import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
from typing import List, Dict, Any
from loguru import logger

from .base_crawler import BaseCrawler, NewItem

class RssCrawler(BaseCrawler):
    """å„ªåŒ–å¾Œçš„RSSè¨‚é–±æºçˆ¬èŸ² - å°ˆæ³¨ä¿éšªæ–°è"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.hours_limit = config.get('hours_limit', 24)
        
        # RSSè¨‚é–±æº
        self.rss_feeds = config.get('rss_feeds', [
            # ä¸»æµè²¡ç¶“åª’é«”
            "https://ec.ltn.com.tw/rss/finance.xml",            # è‡ªç”±æ™‚å ±è²¡ç¶“
            "https://www.chinatimes.com/rss/finance.xml",       # ä¸­åœ‹æ™‚å ±è²¡ç¶“
            "https://news.cnyes.com/rss/news/cat/tw_stock",     # é‰…äº¨ç¶²å°è‚¡
            "https://news.cnyes.com/rss/news/cat/tw_macro",     # é‰…äº¨ç¶²å°ç£ç¸½ç¶“
            "https://udn.com/rssfeed/news/2/6638?ch=news",      # è¯åˆå ±é‡‘èè¦è
            "https://money.udn.com/rssfeed/news/1001/5590/12017?ch=money",  # ç¶“æ¿Ÿæ—¥å ±è²¡ç¶“
            "https://ctee.com.tw/feed",                         # å·¥å•†æ™‚å ±
            "https://www.wealth.com.tw/rss/category/4",         # è²¡è¨Šå¿«å ±
        ])
        
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7"
        }
        
        # ä¿éšªé—œéµè©ï¼ˆæŒ‰å„ªå…ˆç´šæ’åºï¼‰
        self.primary_keywords = [
            # å…¬å¸åç¨±ï¼ˆæœ€é«˜å„ªå…ˆç´šï¼‰
            "æ–°å…‰äººå£½", "æ–°å…‰é‡‘æ§", "æ–°å…‰é‡‘", "å°æ–°äººå£½", "å°æ–°é‡‘æ§", "å°æ–°é‡‘",
            
            # å…·é«”éšªç¨®ï¼ˆé«˜å„ªå…ˆç´šï¼‰
            "å¥åº·éšª", "é†«ç™‚éšª", "ç™Œç—‡éšª", "é‡å¤§ç–¾ç—…éšª", "å¯¦æ”¯å¯¦ä»˜",
            "æŠ•è³‡å‹ä¿éšª", "æŠ•è³‡å‹", "è®Šé¡ä¿éšª", "åˆ©è®Šå£½éšª", "åˆ©ç‡è®Šå‹•å‹",
            "æ„å¤–éšª", "å‚·å®³éšª", "å¹´é‡‘éšª", "å„²è“„éšª", "çµ‚èº«å£½éšª"
        ]
        
        self.secondary_keywords = [
            # ä¿éšªæ¥­å‹™è©å½™
            "ä¿éšª", "å£½éšª", "ç†è³ ", "çµ¦ä»˜", "ä¿å–®", "ä¿è²»", "æ‰¿ä¿", "æ ¸ä¿",
            "è¦ä¿äºº", "è¢«ä¿éšªäºº", "å—ç›Šäºº", "ä¿éšªé‡‘é¡", "ä¿éšœé¡åº¦",
            "ä¿éšªæ¥­", "ä¿éšªå…¬å¸", "ä¿éšªæ³•", "ä¿éšªç›£ç†"
        ]
        
        # æ’é™¤é—œéµè©
        self.exclude_keywords = [
            "è‚¡åƒ¹", "è‚¡ç¥¨", "é…æ¯", "é™¤æ¬Š", "é™¤æ¯", "è‚¡æ±æœƒ",
            "ETF", "åŸºé‡‘", "å‚µåˆ¸", "åŒ¯ç‡", "å¤®è¡Œ", "å‡æ¯", "é™æ¯"
        ]
    
    def crawl(self) -> List[NewItem]:
        """çˆ¬å–RSSè¨‚é–±æºçš„æ–°è"""
        all_news = []
        
        for feed_url in self.rss_feeds:
            logger.info(f"ğŸ“¡ æ­£åœ¨çˆ¬å–RSS: {feed_url}")
            try:
                news_items = self._parse_feed(feed_url)
                all_news.extend(news_items)
                
                # é¿å…éåº¦è«‹æ±‚
                time.sleep(2)
            except Exception as e:
                logger.error(f"âŒ çˆ¬å–RSS '{feed_url}' æ™‚å‡ºéŒ¯: {str(e)}")
        
        # é€²éšç¯©é¸é‚è¼¯
        filtered_news = []
        logger.info(f"ğŸ” é–‹å§‹ç¯©é¸ {len(all_news)} æ¢æ–°è...")
        
        for item in all_news:
            title_content = (item.title + " " + (item.content or "")).lower()
            
            # æª¢æŸ¥æ’é™¤é—œéµè©
            contains_exclude = any(exclude_word in title_content for exclude_word in self.exclude_keywords)
            if contains_exclude:
                logger.debug(f"âŒ æ’é™¤æ–°èï¼ˆåŒ…å«æ’é™¤é—œéµè©ï¼‰: {item.title[:30]}...")
                continue
            
            matched_keyword = None
            priority_score = 0
            
            # æª¢æŸ¥ä¸»è¦é—œéµè©ï¼ˆæœ€é«˜å„ªå…ˆç´šï¼‰
            for keyword in self.primary_keywords:
                if keyword in item.title or (item.content and keyword in item.content):
                    matched_keyword = keyword
                    priority_score = 10
                    break
            
            # æª¢æŸ¥æ¬¡è¦é—œéµè©
            if not matched_keyword:
                for keyword in self.secondary_keywords:
                    if keyword in item.title or (item.content and keyword in item.content):
                        matched_keyword = keyword
                        priority_score = 5
                        break
            
            # æª¢æŸ¥åŸå§‹æœå°‹é—œéµè©
            if not matched_keyword:
                for term in self.search_terms:
                    if term in item.title or (item.content and term in item.content):
                        matched_keyword = term
                        priority_score = 1
                        break
            
            if matched_keyword:
                item.keyword = matched_keyword
                item.priority_score = priority_score
                filtered_news.append(item)
                logger.info(f"âœ… ç¬¦åˆé—œéµè© '{matched_keyword}' (å„ªå…ˆç´š:{priority_score}): {item.title[:40]}...")
        
        logger.info(f"ğŸ¯ ç¯©é¸å®Œæˆï¼Œå‰©é¤˜ {len(filtered_news)} æ¢ç›¸é—œæ–°è")
        
        # æŒ‰å„ªå…ˆç´šå’Œæ™‚é–“æ’åº
        sorted_news = sorted(filtered_news, key=lambda x: (-getattr(x, 'priority_score', 0), -x.published_time.timestamp()))
        return sorted_news[:15]  # è¿”å›å‰15æ¢æœ€ç›¸é—œçš„æ–°è
    
    def _parse_feed(self, feed_url: str) -> List[NewItem]:
        """è§£æRSSè¨‚é–±æº"""
        news_items = []
        
        try:
            # è¨­ç½®User-Agent
            feedparser.USER_AGENT = self.headers["User-Agent"]
            
            # è§£æRSSè¨‚é–±æº
            feed = feedparser.parse(feed_url)
            
            if feed.bozo:
                logger.warning(f"âš ï¸ RSSè¨‚é–±æºå¯èƒ½æœ‰æ ¼å¼å•é¡Œ: {feed_url}")
            
            feed_title = feed.feed.title if hasattr(feed.feed, 'title') else "æœªçŸ¥ä¾†æº"
            logger.info(f"ğŸ“° æ­£åœ¨è™•ç† {feed_title} çš„ {len(feed.entries)} æ¢æ–°è")
            
            processed_count = 0
            insurance_related_count = 0
            
            for entry in feed.entries:
                try:
                    # ç²å–æ¨™é¡Œå’Œé€£çµ
                    title = entry.title if hasattr(entry, 'title') else ""
                    url = entry.link if hasattr(entry, 'link') else ""
                    
                    if not title or not url:
                        continue
                    
                    processed_count += 1
                    
                    # é ç¯©é¸ï¼šåªè™•ç†åŒ…å«ä¿éšªç›¸é—œè©å½™çš„æ¨™é¡Œ
                    title_lower = title.lower()
                    contains_insurance_keyword = any(keyword in title_lower for keyword in [
                        "ä¿éšª", "å£½éšª", "æ–°å…‰", "å°æ–°", "ç†è³ ", "ä¿å–®", "ä¿è²»", 
                        "å¥åº·éšª", "æ„å¤–éšª", "é†«ç™‚éšª", "æŠ•ä¿", "æ‰¿ä¿", "çµ¦ä»˜",
                        "æŠ•è³‡å‹", "åˆ©è®Š", "å¹´é‡‘", "å„²è“„éšª", "é‡å¤§ç–¾ç—…", "ç™Œç—‡éšª"
                    ])
                    
                    # å¦‚æœæ¨™é¡Œä¸åŒ…å«ä¿éšªç›¸é—œè©å½™ï¼Œè·³é
                    if not contains_insurance_keyword:
                        continue
                    
                    insurance_related_count += 1
                    logger.debug(f"ğŸ¯ ç™¼ç¾ä¿éšªç›¸é—œæ–°è: {title[:50]}...")
                    
                    # ç²å–ç™¼å¸ƒæ™‚é–“
                    pub_time = datetime.now()
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        try:
                            pub_time = datetime(*entry.published_parsed[:6])
                        except:
                            pass
                    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                        try:
                            pub_time = datetime(*entry.updated_parsed[:6])
                        except:
                            pass
                    
                    # æª¢æŸ¥æ™‚é–“é™åˆ¶
                    hours_diff = (datetime.now() - pub_time).total_seconds() / 3600
                    logger.debug(f"ğŸ“… æ–°èæ™‚é–“: {pub_time}, è·ç¾åœ¨: {hours_diff:.1f} å°æ™‚")
                    
                    if hours_diff > self.hours_limit:
                        logger.debug(f"â° è·³éï¼Œè¶…å‡ºæ™‚é–“é™åˆ¶: {self.hours_limit} å°æ™‚")
                        continue
                    
                    # ç²å–å…§å®¹
                    content = ""
                    if hasattr(entry, 'content') and entry.content:
                        content = entry.content[0].value
                    elif hasattr(entry, 'summary'):
                        content = entry.summary
                    elif hasattr(entry, 'description'):
                        content = entry.description
                    
                    # å¦‚æœå…§å®¹æ˜¯HTMLï¼Œæ¸…ç†ç‚ºç´”æ–‡æœ¬
                    if content and "<" in content:
                        soup = BeautifulSoup(content, 'html.parser')
                        content = soup.get_text()
                    
                    # å¦‚æœå…§å®¹ç‚ºç©ºæˆ–å¤ªçŸ­ï¼Œå˜—è©¦å¾åŸå§‹é é¢ç²å–
                    if not content or len(content) < 50:
                        content = self._get_article_content(url)
                    
                    # æ¸…ç†æ¨™é¡Œ
                    title = title.replace('\n', ' ').replace('\r', ' ').strip()
                    title = ''.join(char for char in title if ord(char) < 65536)
                    
                    # å‰µå»ºæ–°èé …ç›®
                    news_item = NewItem(
                        title=title,
                        content=content,
                        url=url,
                        published_time=pub_time,
                        source=feed_title,
                        keyword=""
                    )
                    
                    news_items.append(news_item)
                    logger.debug(f"âœ… æˆåŠŸè§£æRSSæ¢ç›®: {title[:30]}...")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ è§£æRSSæ¢ç›®æ™‚å‡ºéŒ¯: {str(e)}")
            
            logger.info(f"ğŸ“Š {feed_title}: è™•ç†äº†{processed_count}æ¢æ–°èï¼Œæ‰¾åˆ°{insurance_related_count}æ¢ä¿éšªç›¸é—œï¼ŒæˆåŠŸè§£æ{len(news_items)}æ¢")
            
        except Exception as e:
            logger.error(f"âŒ è§£æRSSè¨‚é–±æº '{feed_url}' æ™‚å‡ºéŒ¯: {str(e)}")
        
        return news_items
    
    def _get_article_content(self, url: str) -> str:
        """ç²å–æ–‡ç« å…§å®¹"""
        try:
            # è¨­ç½®å»¶é²ï¼Œé¿å…è¢«å°é–
            time.sleep(1)
            
            response = requests.get(url, headers=self.headers, timeout=15)
            response.raise_for_status()
            
            # è‡ªå‹•æª¢æ¸¬ç·¨ç¢¼
            if response.apparent_encoding:
                response.encoding = response.apparent_encoding
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç§»é™¤è…³æœ¬å’Œæ¨£å¼æ¨™ç±¤
            for script in soup(["script", "style", "iframe", "ins", ".ad", ".ads"]):
                script.extract()
            
            # å˜—è©¦å¤šç¨®å…§å®¹é¸æ“‡å™¨
            content_selectors = [
                "div.article-content", 
                "div.article-body",
                "div.story-content",
                "div.news-content",
                "div.cont",
                "div.content",
                "article",
                "main",
                ".news-detail",
                ".article",
                ".post-content",
                ".entry-content"
            ]
            
            content_text = ""
            for selector in content_selectors:
                content_element = soup.select_one(selector)
                if content_element:
                    content_text = content_element.get_text(separator="\n").strip()
                    if len(content_text) > 100:
                        break
            
            # å¦‚æœæ‰¾ä¸åˆ°å…§å®¹ï¼Œä½¿ç”¨æ›´é€šç”¨çš„æ–¹æ³•
            if not content_text or len(content_text) < 100:
                # ç§»é™¤é ­éƒ¨ã€åº•éƒ¨ç­‰ç„¡é—œå…ƒç´ 
                for element in soup.select("header, footer, nav, aside, .sidebar, .ads, .ad"):
                    element.extract()
                
                content_text = soup.get_text(separator="\n").strip()
                lines = [line.strip() for line in content_text.splitlines() if line.strip()]
                content_text = "\n".join(lines)
            
            # æ¸…ç†æ–‡æœ¬
            if content_text:
                content_text = content_text.replace('\n', ' ').replace('\r', ' ').strip()
                content_text = ''.join(char for char in content_text if ord(char) < 65536)
            
            return content_text
            
        except Exception as e:
            logger.warning(f"âš ï¸ ç²å–æ–‡ç« å…§å®¹æ™‚å‡ºéŒ¯: {str(e)}")
            return "ç„¡æ³•ç²å–æ–‡ç« å…§å®¹"
