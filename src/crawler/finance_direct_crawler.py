import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
import random
from typing import List, Dict, Any
from urllib.parse import urljoin
from loguru import logger
import chardet

from .base_crawler import BaseCrawler, NewItem

class FinanceNewsDirectCrawler(BaseCrawler):
    """æ”¾å¯¬æ¢ä»¶çš„è²¡ç¶“æ–°èç›´æ¥çˆ¬èŸ²"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.hours_limit = config.get('hours_limit', 24)
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7",
            "Connection": "keep-alive"
        }
        
        self.sites = [
            {
                "name": "é‰…äº¨ç¶²-å°è‚¡",
                "url": "https://news.cnyes.com/news/cat/tw_stock",
                "article_selector": "a",
                "base_url": "https://news.cnyes.com"
            },
            {
                "name": "ç¶“æ¿Ÿæ—¥å ±-è²¡ç¶“", 
                "url": "https://money.udn.com/money/cate/12017",
                "article_selector": "a",
                "base_url": "https://money.udn.com"
            },
            {
                "name": "ç¶“æ¿Ÿæ—¥å ±-é‡‘è",
                "url": "https://money.udn.com/money/cate/12016", 
                "article_selector": "a",
                "base_url": "https://money.udn.com"
            },
            {
                "name": "è‡ªç”±è²¡ç¶“",
                "url": "https://ec.ltn.com.tw/",
                "article_selector": "a",
                "base_url": "https://ec.ltn.com.tw"
            },
            {
                "name": "å·¥å•†æ™‚å ±-è²¡ç¶“",
                "url": "https://ctee.com.tw/category/financial",
                "article_selector": "a",
                "base_url": "https://ctee.com.tw"
            }
        ]
        
        # æ”¾å¯¬é—œéµè©æ¢ä»¶
        self.broad_keywords = [
            # å…¬å¸ç›¸é—œï¼ˆæ”¾å¯¬ï¼‰
            "æ–°å…‰", "å°æ–°", "æ–°å…‰äººå£½", "å°æ–°äººå£½", "æ–°å…‰é‡‘æ§", "å°æ–°é‡‘æ§", "æ–°å…‰é‡‘", "å°æ–°é‡‘",
            
            # ä¿éšªç›¸é—œï¼ˆæ”¾å¯¬ï¼‰
            "ä¿éšª", "å£½éšª", "äººå£½", "å¥åº·éšª", "é†«ç™‚éšª", "æ„å¤–éšª", "æŠ•è³‡å‹", "å¹´é‡‘", "å„²è“„éšª",
            "ç†è³ ", "çµ¦ä»˜", "ä¿å–®", "ä¿è²»", "æ‰¿ä¿", "æ ¸ä¿", "è¦ä¿", "è¢«ä¿éšª", "å—ç›Šäºº",
            
            # é‡‘èç›¸é—œï¼ˆæ–°å¢ï¼‰
            "é‡‘æ§", "é‡‘è", "ä¿éšœ", "é¢¨éšª"
        ]
        
        # å¤§å¹…æ¸›å°‘æ’é™¤é—œéµè©
        self.exclude_keywords = [
            "è‚¡æ±å¤§æœƒæ±ºè­°", "é…æ¯é™¤æ¯å…¬å‘Š", "è²¡å ±æ³•èªªæœƒ"
        ]
    
    def _detect_encoding(self, content_bytes):
        """æª¢æ¸¬ç·¨ç¢¼"""
        try:
            encodings = ['utf-8', 'big5', 'gb2312']
            for encoding in encodings:
                try:
                    decoded = content_bytes.decode(encoding)
                    return encoding
                except:
                    continue
            return 'utf-8'
        except:
            return 'utf-8'
    
    def crawl(self) -> List[NewItem]:
        """çˆ¬å–è²¡ç¶“æ–°èç¶²ç«™çš„æ–°è"""
        all_news = []
        
        for site in self.sites:
            site_name = site["name"]
            logger.info(f"ğŸ¢ æ­£åœ¨çˆ¬å–ç¶²ç«™: {site_name}")
            
            try:
                news_items = self._crawl_site(site)
                all_news.extend(news_items)
                time.sleep(random.uniform(1, 3))
                logger.info(f"âœ… å¾ {site_name} çˆ¬å–åˆ° {len(news_items)} æ¢æ–°è")
            except Exception as e:
                logger.error(f"âŒ çˆ¬å– {site_name} æ™‚å‡ºéŒ¯: {str(e)}")
        
        logger.info(f"ğŸ“Š è²¡ç¶“ç›´æ¥çˆ¬èŸ²ç¸½è¨ˆç²å¾— {len(all_news)} æ¢åŸå§‹æ–°è")
        
        # æ”¾å¯¬ç¯©é¸é‚è¼¯
        filtered_news = []
        
        for item in all_news:
            title_content = (item.title + " " + (item.content or "")).lower()
            
            # æª¢æŸ¥æ’é™¤é—œéµè©ï¼ˆå¤§å¹…æ¸›å°‘ï¼‰
            contains_exclude = any(exclude_word in title_content for exclude_word in self.exclude_keywords)
            if contains_exclude:
                logger.debug(f"âŒ æ’é™¤æ–°è: {item.title[:30]}...")
                continue
            
            matched_keyword = None
            priority_score = 1  # é è¨­åˆ†æ•¸
            
            # æª¢æŸ¥æ˜¯å¦åŒ…å«ä»»ä½•ç›¸é—œé—œéµè©
            for keyword in self.broad_keywords:
                if keyword in item.title or (item.content and keyword in item.content):
                    matched_keyword = keyword
                    
                    # è¨­å®šå„ªå…ˆç´š
                    if keyword in ["æ–°å…‰äººå£½", "å°æ–°äººå£½", "æ–°å…‰é‡‘æ§", "å°æ–°é‡‘æ§"]:
                        priority_score = 10
                    elif keyword in ["æ–°å…‰", "å°æ–°"]:
                        priority_score = 8
                    elif keyword in ["å¥åº·éšª", "é†«ç™‚éšª", "æŠ•è³‡å‹", "ç†è³ "]:
                        priority_score = 6
                    elif keyword in ["ä¿éšª", "å£½éšª", "äººå£½"]:
                        priority_score = 4
                    else:
                        priority_score = 2
                    break
            
            # å¦‚æœæ‰¾åˆ°ä»»ä½•åŒ¹é…ï¼Œå°±åŠ å…¥
            if matched_keyword:
                item.keyword = matched_keyword
                item.priority_score = priority_score
                filtered_news.append(item)
                logger.info(f"âœ… ç¬¦åˆé—œéµè© '{matched_keyword}' (å„ªå…ˆç´š:{priority_score}): {item.title[:40]}...")
        
        logger.info(f"ğŸ¯ è²¡ç¶“ç›´æ¥çˆ¬èŸ²ç¯©é¸å®Œæˆï¼Œå‰©é¤˜ {len(filtered_news)} æ¢ç›¸é—œæ–°è")
        
        # æŒ‰å„ªå…ˆç´šæ’åº
        sorted_news = sorted(filtered_news, key=lambda x: (-getattr(x, 'priority_score', 0), -x.published_time.timestamp()))
        return sorted_news[:30]  # å¢åŠ è¿”å›æ•¸é‡
    
    def _crawl_site(self, site: Dict[str, Any]) -> List[NewItem]:
        """çˆ¬å–ç‰¹å®šç¶²ç«™çš„æ–°è"""
        news_items = []
        
        try:
            response = requests.get(site["url"], headers=self.headers, timeout=15)
            response.raise_for_status()
            
            if response.content:
                detected_encoding = self._detect_encoding(response.content)
                response.encoding = detected_encoding
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æ‰¾æ‰€æœ‰é€£çµ
            article_elements = soup.find_all("a", href=True)
            
            processed_count = 0
            related_count = 0
            
            for element in article_elements:
                if processed_count >= 200:  # å¢åŠ è™•ç†æ•¸é‡
                    break
                    
                try:
                    # ç²å–æ¨™é¡Œ
                    title = element.get_text().strip()
                    
                    if not title or len(title) < 5:
                        continue
                    
                    # æ¸…ç†æ¨™é¡Œ
                    title = title.replace('\n', ' ').replace('\r', ' ').strip()
                    title = ''.join(char for char in title if ord(char) < 65536)
                    
                    processed_count += 1
                    
                    # æ”¾å¯¬é ç¯©é¸æ¢ä»¶
                    title_lower = title.lower()
                    
                    # æª¢æŸ¥æ˜¯å¦åŒ…å«ä»»ä½•ç›¸é—œé—œéµè©
                    contains_related = any(keyword in title_lower for keyword in [
                        "ä¿éšª", "å£½éšª", "äººå£½", "æ–°å…‰", "å°æ–°", "ç†è³ ", "ä¿å–®", "ä¿è²»", 
                        "å¥åº·éšª", "é†«ç™‚éšª", "æ„å¤–éšª", "æŠ•ä¿", "æ‰¿ä¿", "çµ¦ä»˜", "é‡‘æ§",
                        "æŠ•è³‡å‹", "åˆ©è®Š", "å¹´é‡‘", "å„²è“„éšª", "é¢¨éšª", "ä¿éšœ"
                    ])
                    
                    # åªè¦åŒ…å«ä»»ä½•ç›¸é—œè©å½™å°±è™•ç†
                    if not contains_related:
                        continue
                    
                    related_count += 1
                    
                    # ç²å–é€£çµ
                    url = element.get("href")
                    if not url:
                        continue
                    
                    if not url.startswith(("http://", "https://")):
                        url = urljoin(site["base_url"], url)
                    
                    # è¨­å®šç™¼å¸ƒæ™‚é–“
                    pub_time = datetime.now()
                    
                    # ç²å–å…§å®¹ï¼ˆç°¡åŒ–ï¼‰
                    content = title  # æš«æ™‚ä½¿ç”¨æ¨™é¡Œä½œç‚ºå…§å®¹ï¼Œé¿å…éåº¦è«‹æ±‚
                    
                    # å‰µå»ºæ–°èé …ç›®
                    news_item = NewItem(
                        title=title,
                        content=content,
                        url=url,
                        published_time=pub_time,
                        source=site["name"],
                        keyword=""
                    )
                    
                    news_items.append(news_item)
                    
                    # é™åˆ¶æ¯å€‹ç¶²ç«™çš„æ–°èæ•¸é‡
                    if len(news_items) >= 50:
                        break
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ è§£ææ–‡ç« æ™‚å‡ºéŒ¯: {str(e)}")
            
            logger.info(f"ğŸ“Š {site['name']}: è™•ç†äº†{processed_count}ç¯‡æ–‡ç« ï¼Œæ‰¾åˆ°{related_count}ç¯‡ç›¸é—œï¼ŒæˆåŠŸè§£æ{len(news_items)}ç¯‡")
            
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–ç¶²ç«™ {site['name']} æ™‚å‡ºéŒ¯: {str(e)}")
        
        return news_items
