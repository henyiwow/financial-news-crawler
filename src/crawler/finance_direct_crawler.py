import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
import random
from typing import List, Dict, Any
from urllib.parse import urljoin
from loguru import logger
import chardet

from .base_crawler import BaseCrawler, NewItem

class FinanceNewsDirectCrawler(BaseCrawler):
    """å„ªåŒ–å¾Œçš„è²¡ç¶“æ–°èç¶²ç«™ç›´æ¥çˆ¬èŸ² - å°ˆæ³¨ä¿éšªæ–°è"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.hours_limit = config.get('hours_limit', 24)
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Charset": "utf-8, big5, gb2312, iso-8859-1",
            "Connection": "keep-alive"
        }
        
        # æ“´å……ä¿éšªç›¸é—œæ–°èç¶²ç«™
        self.sites = [
            {
                "name": "é‰…äº¨ç¶²-å°è‚¡",
                "url": "https://news.cnyes.com/news/cat/tw_stock",
                "article_selector": "a.theme-list-title, a[href*='news']",
                "title_selector": "h3, h2, .title",
                "base_url": "https://news.cnyes.com"
            },
            {
                "name": "ç¶“æ¿Ÿæ—¥å ±-è²¡ç¶“", 
                "url": "https://money.udn.com/money/cate/12017",
                "article_selector": "a[href*='story'], a[href*='news']",
                "title_selector": "h3, h2, .title",
                "base_url": "https://money.udn.com"
            },
            {
                "name": "ç¶“æ¿Ÿæ—¥å ±-é‡‘è",
                "url": "https://money.udn.com/money/cate/12016", 
                "article_selector": "a[href*='story'], a[href*='news']",
                "title_selector": "h3, h2, .title",
                "base_url": "https://money.udn.com"
            },
            {
                "name": "è‡ªç”±è²¡ç¶“",
                "url": "https://ec.ltn.com.tw/",
                "article_selector": "a.tit, a[href*='news']",
                "title_selector": "self",
                "base_url": "https://ec.ltn.com.tw"
            },
            {
                "name": "å·¥å•†æ™‚å ±-è²¡ç¶“",
                "url": "https://ctee.com.tw/category/financial",
                "article_selector": "a[href*='news'], .post-title a",
                "title_selector": "self", 
                "base_url": "https://ctee.com.tw"
            },
            {
                "name": "MoneyDJç†è²¡ç¶²",
                "url": "https://www.moneydj.com/",
                "article_selector": "a[href*='news']",
                "title_selector": "self",
                "base_url": "https://www.moneydj.com"
            }
        ]
        
        # ä¿éšªé—œéµè©ï¼ˆæŒ‰å„ªå…ˆç´šæ’åºï¼‰
        self.primary_keywords = [
            # å…¬å¸åç¨±ï¼ˆæœ€é«˜å„ªå…ˆç´šï¼‰
            "æ–°å…‰äººå£½", "æ–°å…‰é‡‘æ§", "æ–°å…‰é‡‘", "å°æ–°äººå£½", "å°æ–°é‡‘æ§", "å°æ–°é‡‘",
            
            # å…·é«”éšªç¨®ï¼ˆé«˜å„ªå…ˆç´šï¼‰
            "å¥åº·éšª", "é†«ç™‚éšª", "ç™Œç—‡éšª", "é‡å¤§ç–¾ç—…éšª", "å¯¦æ”¯å¯¦ä»˜",
            "æŠ•è³‡å‹ä¿éšª", "æŠ•è³‡å‹", "è®Šé¡ä¿éšª", "åˆ©è®Šå£½éšª", "åˆ©ç‡è®Šå‹•å‹",
            "æ„å¤–éšª", "å‚·å®³éšª", "å¹´é‡‘éšª", "å„²è“„éšª", "çµ‚èº«å£½éšª"
        ]
        
        self.secondary_keywords = [
            # ä¿éšªæ¥­å‹™è©å½™
            "ä¿éšª", "å£½éšª", "ç†è³ ", "çµ¦ä»˜", "ä¿å–®", "ä¿è²»", "æ‰¿ä¿", "æ ¸ä¿",
            "è¦ä¿äºº", "è¢«ä¿éšªäºº", "å—ç›Šäºº", "ä¿éšªé‡‘é¡", "ä¿éšœé¡åº¦"
        ]
        
        # æ’é™¤é—œéµè©
        self.exclude_keywords = [
            "è‚¡åƒ¹", "è‚¡ç¥¨", "é…æ¯", "é™¤æ¬Š", "é™¤æ¯", "è‚¡æ±æœƒ", "è‚¡æ±å¤§æœƒ",
            "ETF", "åŸºé‡‘", "å‚µåˆ¸", "åŒ¯ç‡", "å¤®è¡Œæ”¿ç­–", "å‡æ¯", "é™æ¯"
        ]
    
    def _detect_encoding(self, content_bytes):
        """æª¢æ¸¬ä¸¦è¿”å›æ­£ç¢ºçš„ç·¨ç¢¼"""
        try:
            encodings = ['utf-8', 'big5', 'gb2312', 'gbk', 'utf-16', 'iso-8859-1']
            
            for encoding in encodings:
                try:
                    decoded = content_bytes.decode(encoding)
                    if any('\u4e00' <= char <= '\u9fff' for char in decoded[:100]):
                        return encoding
                except (UnicodeDecodeError, UnicodeError):
                    continue
            
            detected = chardet.detect(content_bytes)
            if detected and detected['confidence'] > 0.7:
                return detected['encoding']
            
            return 'utf-8'
        except:
            return 'utf-8'
    
    def crawl(self) -> List[NewItem]:
        """çˆ¬å–è²¡ç¶“æ–°èç¶²ç«™çš„æ–°è"""
        all_news = []
        
        for site in self.sites:
            site_name = site["name"]
            logger.info(f"ğŸ¢ æ­£åœ¨çˆ¬å–ç¶²ç«™: {site_name}")
            
            try:
                news_items = self._crawl_site(site)
                all_news.extend(news_items)
                
                time.sleep(random.uniform(1, 3))
                
                logger.info(f"âœ… å¾ {site_name} çˆ¬å–åˆ° {len(news_items)} æ¢æ–°è")
            except Exception as e:
                logger.error(f"âŒ çˆ¬å– {site_name} æ™‚å‡ºéŒ¯: {str(e)}")
        
        # é€²éšç¯©é¸é‚è¼¯
        filtered_news = []
        logger.info(f"ğŸ” é–‹å§‹ç¯©é¸ {len(all_news)} æ¢æ–°è...")
        
        for item in all_news:
            title_content = (item.title + " " + (item.content or "")).lower()
            
            # æª¢æŸ¥æ’é™¤é—œéµè©
            contains_exclude = any(exclude_word in title_content for exclude_word in self.exclude_keywords)
            if contains_exclude:
                logger.debug(f"âŒ æ’é™¤æ–°èï¼ˆåŒ…å«æ’é™¤é—œéµè©ï¼‰: {item.title[:30]}...")
                continue
            
            matched_keyword = None
            priority_score = 0
            
            # æª¢æŸ¥ä¸»è¦é—œéµè©ï¼ˆæœ€é«˜å„ªå…ˆç´šï¼‰
            for keyword in self.primary_keywords:
                if keyword in item.title or (item.content and keyword in item.content):
                    matched_keyword = keyword
                    priority_score = 10
                    break
            
            # æª¢æŸ¥æ¬¡è¦é—œéµè©
            if not matched_keyword:
                for keyword in self.secondary_keywords:
                    if keyword in item.title or (item.content and keyword in item.content):
                        matched_keyword = keyword
                        priority_score = 5
                        break
            
            # æª¢æŸ¥åŸå§‹æœå°‹é—œéµè©
            if not matched_keyword:
                for term in self.search_terms:
                    if term in item.title or (item.content and term in item.content):
                        matched_keyword = term
                        priority_score = 1
                        break
            
            if matched_keyword:
                item.keyword = matched_keyword
                item.priority_score = priority_score
                filtered_news.append(item)
                logger.info(f"âœ… ç¬¦åˆé—œéµè© '{matched_keyword}' (å„ªå…ˆç´š:{priority_score}): {item.title[:40]}...")
        
        logger.info(f"ğŸ¯ ç¯©é¸å®Œæˆï¼Œå‰©é¤˜ {len(filtered_news)} æ¢ç›¸é—œæ–°è")
        
        # æŒ‰å„ªå…ˆç´šå’Œæ™‚é–“æ’åº
        sorted_news = sorted(filtered_news, key=lambda x: (-getattr(x, 'priority_score', 0), -x.published_time.timestamp()))
        return sorted_news[:20]  # è¿”å›å‰20æ¢æœ€ç›¸é—œçš„æ–°è
    
    def _crawl_site(self, site: Dict[str, Any]) -> List[NewItem]:
        """çˆ¬å–ç‰¹å®šç¶²ç«™çš„æ–°è"""
        news_items = []
        
        try:
            response = requests.get(site["url"], headers=self.headers, timeout=15)
            response.raise_for_status()
            
            if response.content:
                detected_encoding = self._detect_encoding(response.content)
                response.encoding = detected_encoding
                logger.debug(f"ğŸ”¤ æª¢æ¸¬åˆ°ç·¨ç¢¼: {detected_encoding}")
            
            soup = BeautifulSoup(response.text, 'html.parser', from_encoding=response.encoding)
            
            article_elements = soup.select(site["article_selector"])
            
            if not article_elements:
                article_elements = soup.find_all("a", href=True)
                logger.debug(f"ğŸ” ä½¿ç”¨é€šç”¨é¸æ“‡å™¨æ‰¾åˆ° {len(article_elements)} å€‹å€™é¸æ–‡ç« ")
            else:
                logger.debug(f"ğŸ” æ‰¾åˆ° {len(article_elements)} å€‹å€™é¸æ–‡ç« ")
            
            processed_count = 0
            insurance_related_count = 0
            
            for element in article_elements:
                if processed_count >= 100:  # å¢åŠ è™•ç†æ•¸é‡
                    break
                    
                try:
                    # ç²å–æ¨™é¡Œ
                    title = None
                    if site["title_selector"] == "self":
                        title = element.get_text().strip()
                    else:
                        for selector in site["title_selector"].split(", "):
                            title_element = element.select_one(selector)
                            if title_element:
                                title = title_element.get_text().strip()
                                break
                        
                        if not title:
                            title = element.get_text().strip()
                    
                    if title:
                        title = title.replace('\n', ' ').replace('\r', ' ').strip()
                        title = ''.join(char for char in title if ord(char) < 65536)
                    
                    if not title or len(title) < 5:
                        continue
                    
                    processed_count += 1
                    
                    # é ç¯©é¸ï¼šæª¢æŸ¥æ¨™é¡Œæ˜¯å¦åŒ…å«ä¿éšªç›¸é—œè©å½™
                    title_lower = title.lower()
                    
                    # æª¢æŸ¥æ˜¯å¦åŒ…å«ä¿éšªé—œéµè©
                    contains_insurance = any(keyword in title_lower for keyword in [
                        "ä¿éšª", "å£½éšª", "æ–°å…‰", "å°æ–°", "ç†è³ ", "ä¿å–®", "ä¿è²»", 
                        "å¥åº·éšª", "æ„å¤–éšª", "é†«ç™‚éšª", "æŠ•ä¿", "æ‰¿ä¿", "çµ¦ä»˜",
                        "æŠ•è³‡å‹", "åˆ©è®Š", "å¹´é‡‘", "å„²è“„éšª", "é‡å¤§ç–¾ç—…", "ç™Œç—‡éšª"
                    ])
                    
                    # æª¢æŸ¥æ˜¯å¦åŒ…å«æ’é™¤é—œéµè©
                    contains_exclude = any(exclude_word in title_lower for exclude_word in self.exclude_keywords)
                    
                    # åªè™•ç†ä¿éšªç›¸é—œä¸”ä¸åŒ…å«æ’é™¤é—œéµè©çš„æ–°è
                    if not contains_insurance or contains_exclude:
                        continue
                    
                    insurance_related_count += 1
                    logger.debug(f"ğŸ¯ ç™¼ç¾ä¿éšªç›¸é—œæ–°è: {title[:50]}...")
                    
                    # ç²å–é€£çµ
                    url = element.get("href")
                    if not url:
                        continue
                    
                    if not url.startswith(("http://", "https://")):
                        url = urljoin(site["base_url"], url)
                    
                    # ç²å–ç™¼å¸ƒæ™‚é–“ï¼ˆç°¡åŒ–è™•ç†ï¼‰
                    pub_time = datetime.now()
                    
                    # æª¢æŸ¥æ™‚é–“é™åˆ¶
                    hours_diff = (datetime.now() - pub_time).total_seconds() / 3600
                    if hours_diff > self.hours_limit:
                        continue
                    
                    # ç²å–è©³ç´°å…§å®¹
                    content = self._get_article_content(url)
                    
                    # å‰µå»ºæ–°èé …ç›®
                    news_item = NewItem(
                        title=title,
                        content=content,
                        url=url,
                        published_time=pub_time,
                        source=site["name"],
                        keyword=""
                    )
                    
                    news_items.append(news_item)
                    logger.debug(f"âœ… æˆåŠŸè§£ææ–°è: {title[:30]}...")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ è§£ææ–‡ç« æ™‚å‡ºéŒ¯: {str(e)}")
            
            logger.info(f"ğŸ“Š {site['name']}: è™•ç†äº†{processed_count}ç¯‡æ–‡ç« ï¼Œæ‰¾åˆ°{insurance_related_count}ç¯‡ä¿éšªç›¸é—œï¼ŒæˆåŠŸè§£æ{len(news_items)}ç¯‡")
            
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–ç¶²ç«™ {site['name']} æ™‚å‡ºéŒ¯: {str(e)}")
        
        return news_items
    
    def _get_article_content(self, url: str) -> str:
        """ç²å–æ–‡ç« å…§å®¹"""
        try:
            time.sleep(random.uniform(0.5, 1.5))
            
            response = requests.get(url, headers=self.headers, timeout=15)
            response.raise_for_status()
            
            if response.content:
                detected_encoding = self._detect_encoding(response.content)
                response.encoding = detected_encoding
            
            soup = BeautifulSoup(response.text, 'html.parser', from_encoding=response.encoding)
            
            # ç§»é™¤å»£å‘Šå’Œç„¡é—œå…ƒç´ 
            for element in soup.select("script, style, iframe, ins, .ad, .ads, .advertisement"):
                element.extract()
            
            content_selectors = [
                "div.article-content", 
                "div.article-body",
                "div.story-content",
                "div.news-content",
                "div.cont",
                "div.content",
                "article",
                "main",
                ".news-detail",
                ".article",
                ".post-content"
            ]
            
            content_text = ""
            for selector in content_selectors:
                content_element = soup.select_one(selector)
                if content_element:
                    content_text = content_element.get_text(separator="\n").strip()
                    if len(content_text) > 100:
                        break
            
            if not content_text:
                # ç§»é™¤é ­éƒ¨ã€åº•éƒ¨ç­‰ç„¡é—œå…ƒç´ 
                for element in soup.select("header, footer, nav, aside, .sidebar, .ads, .ad"):
                    element.extract()
                
                content_text = soup.get_text(separator="\n").strip()
                lines = [line.strip() for line in content_text.splitlines() if line.strip()]
                content_text = "\n".join(lines)
            
            if content_text:
                content_text = content_text.replace('\n', ' ').replace('\r', ' ').strip()
                content_text = ''.join(char for char in content_text if ord(char) < 65536)
            
            return content_text
        
        except Exception as e:
            logger.warning(f"âš ï¸ ç²å–æ–‡ç« å…§å®¹æ™‚å‡ºéŒ¯: {str(e)}")
            return "ç„¡æ³•ç²å–æ–‡ç« å…§å®¹"
