é—œæ–¼é€™å°ˆæ¡ˆï¼Œæˆ‘å·²ç¶“åŸ·è¡Œäº†ä¸€æ®µï¼Œè©³ç´°å…§å®¹åœ¨æˆ‘å€‘ä¹‹å‰çš„å°è©±ä¸­ï¼Œå› ç‚ºå°è©±é•·åº¦å·²é”ä¸Šé™ï¼Œæ‰€ä»¥é–‹ä¸€å€‹æ–°çš„å°ˆæ¡ˆã€‚é€™å°ˆæ¡ˆç‚ºåˆ©ç”¨GITHUBä¸‹æŒ‡ä»¤æ¯æ—¥çˆ¬èŸ²ä¿éšªç›¸é—œæ–°èä¸¦é–±è®€æ‘˜è¦å¾Œå·²LINEæ©Ÿå™¨äººç”¨å»£æ’­æ–¹å¼å‚³é€

Edit
æˆ‘äº†è§£æ‚¨çš„å°ˆæ¡ˆæ˜¯ä¸€å€‹è‡ªå‹•åŒ–ä¿éšªæ–°èæœå‹™ç³»çµ±ï¼ŒåŒ…å«ä»¥ä¸‹ä¸»è¦åŠŸèƒ½ï¼š

GitHub Actions è‡ªå‹•åŒ–ï¼šæ¯æ—¥å®šæ™‚åŸ·è¡Œçˆ¬èŸ²
æ–°èçˆ¬èŸ²ï¼šæŠ“å–ä¿éšªç›¸é—œæ–°è
å…§å®¹æ‘˜è¦ï¼šå°æ–°èé€²è¡Œæ‘˜è¦è™•ç†
LINE Bot å»£æ’­ï¼šå°‡æ‘˜è¦é€é LINE æ©Ÿå™¨äººç™¼é€çµ¦ç”¨æˆ¶
ç”±æ–¼ä¹‹å‰çš„å°è©±å·²é”ä¸Šé™ï¼Œæˆ‘ç„¡æ³•çœ‹åˆ°è©³ç´°çš„å¯¦ä½œå…§å®¹ï¼Œä½†æˆ‘å¯ä»¥å”åŠ©æ‚¨ç¹¼çºŒé–‹ç™¼æˆ–è§£æ±ºé‡åˆ°çš„å•é¡Œã€‚

è«‹å‘Šè¨´æˆ‘ï¼š

ç›®å‰å°ˆæ¡ˆé€²è¡Œåˆ°å“ªå€‹éšæ®µï¼Ÿ
é‡åˆ°äº†ä»€éº¼å…·é«”å•é¡Œéœ€è¦è§£æ±ºï¼Ÿ
éœ€è¦æˆ‘å”åŠ©å„ªåŒ–æˆ–æ–°å¢å“ªäº›åŠŸèƒ½ï¼Ÿ
æˆ‘å¯ä»¥å¹«æ‚¨è™•ç†ç¨‹å¼ç¢¼é–‹ç™¼ã€GitHub Actions è¨­å®šã€LINE Bot API æ•´åˆã€æˆ–æ˜¯æ–°èçˆ¬èŸ²å’Œæ‘˜è¦åŠŸèƒ½çš„å„ªåŒ–ç­‰å„æ–¹é¢çš„éœ€æ±‚ã€‚




ç¾åœ¨çš„ç‹€æ³æ˜¯å·²ç¶“å¯ä»¥æ­£å¸¸åŸ·è¡Œï¼Œä½†æ–°èé‡å¤ªå°‘ä¸”éƒ¨æ˜¯æˆ‘æ‰€éœ€è¦çš„æ–°èï¼Œä½ éœ€è¦æˆ‘çµ¦ä½ é‚£äº›è³‡è¨Š?

Edit
ç‚ºäº†å¹«æ‚¨å„ªåŒ–æ–°èçˆ¬èŸ²ï¼Œè®“å®ƒèƒ½æŠ“å–æ›´å¤šä¸”æ›´ç¬¦åˆéœ€æ±‚çš„ä¿éšªæ–°èï¼Œæˆ‘éœ€è¦äº†è§£ä»¥ä¸‹è³‡è¨Šï¼š

ç›®å‰ç‹€æ³
ç¾æœ‰æ–°èä¾†æºï¼šç›®å‰çˆ¬èŸ²å¾å“ªäº›ç¶²ç«™æŠ“å–æ–°èï¼Ÿ
æ¯æ—¥æ–°èæ•¸é‡ï¼šå¤§æ¦‚æ¯å¤©èƒ½æŠ“åˆ°å¹¾å‰‡æ–°èï¼Ÿ
æ–°èå“è³ªå•é¡Œï¼šæŠ“åˆ°çš„æ–°èä¸»è¦æœ‰ä»€éº¼å•é¡Œï¼Ÿï¼ˆä¾‹å¦‚ï¼šä¸æ˜¯ä¿éšªç›¸é—œã€å…§å®¹å¤ªæ·ºã€é‡è¤‡æ€§é«˜ç­‰ï¼‰
éœ€æ±‚è¦æ ¼
ç†æƒ³æ–°èé¡å‹ï¼šæ‚¨å¸Œæœ›çœ‹åˆ°å“ªäº›é¡å‹çš„ä¿éšªæ–°èï¼Ÿ
å£½éšª/ç”¢éšª/å¥åº·éšªç­‰ç‰¹å®šéšªç¨®ï¼Ÿ
ä¿éšªæ³•è¦æ”¿ç­–ï¼Ÿ
ä¿éšªå…¬å¸å‹•æ…‹ï¼Ÿ
ç†è³ æ¡ˆä¾‹ï¼Ÿ
æ–°å•†å“ç™¼å¸ƒï¼Ÿ
æœŸæœ›æ•¸é‡ï¼šæ¯å¤©å¸Œæœ›æ”¶åˆ°å¹¾å‰‡æ–°èï¼Ÿ
æ–°èä¾†æºåå¥½ï¼š
å®˜æ–¹åª’é«”ï¼ˆå¦‚ç¶“æ¿Ÿæ—¥å ±ã€å·¥å•†æ™‚å ±ï¼‰
ä¿éšªå°ˆæ¥­åª’é«”
æ”¿åºœæ©Ÿé—œç™¼å¸ƒ
ç‰¹å®šä¸æƒ³è¦çš„ä¾†æº
æŠ€è¡“è³‡è¨Š
ç›®å‰çš„çˆ¬èŸ²ç¨‹å¼ç¢¼ï¼šå¯ä»¥åˆ†äº«ç¾æœ‰çš„çˆ¬èŸ²é‚è¼¯å—ï¼Ÿ
é—œéµå­—ç¯©é¸ï¼šç›®å‰ç”¨ä»€éº¼é—œéµå­—éæ¿¾æ–°èï¼Ÿ
æœ‰äº†é€™äº›è³‡è¨Šï¼Œæˆ‘å°±èƒ½å¹«æ‚¨æ“´å……æ–°èä¾†æºã€å„ªåŒ–é—œéµå­—ç¯©é¸ï¼Œä¸¦æå‡æ–°èçš„ç›¸é—œæ€§å’Œæ•¸é‡ã€‚




image.png
é€™æ˜¯GITHUBå…§çš„çµå”®

Edit
æ„Ÿè¬æ‚¨æä¾›GitHubå°ˆæ¡ˆçš„æˆªåœ–ï¼æˆ‘å¯ä»¥çœ‹åˆ°é€™æ˜¯ä¸€å€‹ã€Œé‡‘èä¿éšªæ–°èçˆ¬èŸ²èˆ‡Lineæ¨æ’­ã€çš„å°ˆæ¡ˆï¼ŒåŒ…å«äº†ä»¥ä¸‹çµæ§‹ï¼š

.github/workflows/ - GitHub Actions è‡ªå‹•åŒ–æµç¨‹
config/ - è¨­å®šæª”
src/ - åŸå§‹ç¢¼
tests/ - æ¸¬è©¦æª”æ¡ˆ
requirements.txt - Python ä¾è³´å¥—ä»¶
ç‚ºäº†å¹«æ‚¨å„ªåŒ–æ–°èçˆ¬èŸ²ï¼Œè®“å®ƒèƒ½æŠ“å–æ›´å¤šä¸”æ›´ç¬¦åˆéœ€æ±‚çš„ä¿éšªæ–°èï¼Œæˆ‘éœ€è¦æŸ¥çœ‹ä¸€äº›é—œéµæª”æ¡ˆã€‚å¯ä»¥è«‹æ‚¨åˆ†äº«ä»¥ä¸‹æª”æ¡ˆçš„å…§å®¹å—ï¼Ÿ

ä¸»è¦çˆ¬èŸ²ç¨‹å¼ - src/ è³‡æ–™å¤¾ä¸­çš„çˆ¬èŸ²ç¨‹å¼ç¢¼
è¨­å®šæª” - config/ è³‡æ–™å¤¾ä¸­çš„è¨­å®šæª”
GitHub Actions - .github/workflows/ ä¸­çš„è‡ªå‹•åŒ–æµç¨‹æª”æ¡ˆ
å¦å¤–ï¼Œä¹Ÿè«‹å‘Šè¨´æˆ‘ï¼š

ç›®å‰æ¯å¤©å¤§æ¦‚èƒ½æŠ“åˆ°å¹¾å‰‡æ–°èï¼Ÿ
æ‚¨å¸Œæœ›çœ‹åˆ°å“ªäº›é¡å‹çš„ä¿éšªæ–°èï¼Ÿï¼ˆä¾‹å¦‚ï¼šå£½éšªã€ç”¢éšªã€å¥åº·éšªã€ä¿éšªæ³•è¦ã€ç†è³ æ¡ˆä¾‹ç­‰ï¼‰
æœ‰æ²’æœ‰ç‰¹åˆ¥åå¥½çš„æ–°èä¾†æºç¶²ç«™ï¼Ÿ
é€™æ¨£æˆ‘å°±èƒ½å¹«æ‚¨åˆ†æç¾æœ‰çš„çˆ¬èŸ²é‚è¼¯ï¼Œä¸¦å»ºè­°å¦‚ä½•æ“´å……æ–°èä¾†æºå’Œå„ªåŒ–ç¯©é¸æ¢ä»¶ã€‚




import os
import sys
import yaml
import schedule
import time
from datetime import datetime
from typing import List, Dict, Any
from loguru import logger
# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ°ç³»çµ±è·¯å¾‘
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(file))))
from src.crawler.google_news_crawler import GoogleNewsCrawler
from src.crawler.rss_crawler import RssCrawler
from src.crawler.finance_direct_crawler import FinanceNewsDirectCrawler
from src.summarizer.text_summarizer import TextSummarizer
from src.notification.line_notifier import LineNotifier
from src.crawler.utils import load_config, setup_logger
def run_crawler():
    """åŸ·è¡Œçˆ¬èŸ²ã€æ‘˜è¦å’Œé€šçŸ¥æµç¨‹"""
    start_time = datetime.now()
    logger.info(f"é–‹å§‹åŸ·è¡Œçˆ¬èŸ²ä»»å‹™: {start_time}")

    try:
        # è¼‰å…¥é…ç½®
        config_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(file))), 'config', 'config.yaml')
        config = load_config(config_path)

        all_news = []

        # å„ªå…ˆä½¿ç”¨è²¡ç¶“æ–°èç›´æ¥çˆ¬èŸ²ï¼Œå› ç‚ºé€™æ›´æœ‰å¯èƒ½æ‰¾åˆ°ç›¸é—œæ–°è
        if 'finance_direct' in config['crawler']['sources']:
            logger.info("ä½¿ç”¨è²¡ç¶“æ–°èç›´æ¥çˆ¬èŸ²ç²å–æ–°è")
            finance_crawler = FinanceNewsDirectCrawler(config['crawler'])
            finance_news = finance_crawler.crawl()
            all_news.extend(finance_news)
            logger.info(f"å¾è²¡ç¶“æ–°èç¶²ç«™ç›´æ¥çˆ¬å–åˆ° {len(finance_news)} æ¢æ–°è")

        # ä½¿ç”¨Googleæ–°èçˆ¬èŸ²
        if 'google_news' in config['crawler']['sources']:
            logger.info("ä½¿ç”¨Googleæ–°èçˆ¬èŸ²ç²å–æ–°è")
            google_crawler = GoogleNewsCrawler(config['crawler'])
            google_news = google_crawler.crawl()
            all_news.extend(google_news)
            logger.info(f"å¾Googleæ–°èçˆ¬å–åˆ° {len(google_news)} æ¢æ–°è")

        # ä½¿ç”¨RSSçˆ¬èŸ²
        if 'rss' in config['crawler']['sources']:
            logger.info("ä½¿ç”¨RSSçˆ¬èŸ²ç²å–æ–°è")
            rss_crawler = RssCrawler(config['crawler'])
            rss_news = rss_crawler.crawl()
            all_news.extend(rss_news)
            logger.info(f"å¾RSSçˆ¬å–åˆ° {len(rss_news)} æ¢æ–°è")

        # æ ¹æ“šé—œéµè©å„ªå…ˆæ’åºæ‰€æœ‰æ–°è
        priority_map = {term: i for i, term in enumerate(config['crawler']['search_terms'])}
        all_news = sorted(all_news, key=lambda item: priority_map.get(item.keyword, float('inf')))

        logger.info(f"ç¸½å…±çˆ¬å–åˆ° {len(all_news)} æ¢æ–°è")

        if not all_news:
            logger.info("æ²’æœ‰æ‰¾åˆ°ç›¸é—œæ–°èï¼Œä»»å‹™çµæŸ")
            return

        # åˆå§‹åŒ–æ‘˜è¦å™¨
        summarizer = TextSummarizer(config['summarizer'])

        # ç”Ÿæˆæ‘˜è¦
        news_summaries = []
        for item in all_news:
            summary = summarizer.summarize(item.content)

            news_summaries.append({
                'title': item.title,
                'summary': summary,
                'url': item.url,
                'source': item.source,
                'keyword': item.keyword,
                'published_time': item.published_time
            })

            logger.info(f"å·²ç”Ÿæˆæ‘˜è¦: {item.title[:30]}...")

        # åˆå§‹åŒ–Lineé€šçŸ¥
        notifier = LineNotifier(config['line_notify'])

        # ç™¼é€æ‘˜è¦åˆ°Line
        sent = notifier.send_news_summary(news_summaries)
        if sent:
            logger.info("æˆåŠŸç™¼é€Lineé€šçŸ¥")
        else:
            logger.error("ç™¼é€Lineé€šçŸ¥å¤±æ•—")

    except Exception as e:
        logger.error(f"åŸ·è¡Œçˆ¬èŸ²ä»»å‹™æ™‚å‡ºéŒ¯: {str(e)}")

    end_time = datetime.now()
    logger.info(f"çˆ¬èŸ²ä»»å‹™çµæŸï¼Œè€—æ™‚: {end_time - start_time}")
def main():
    """ä¸»å‡½æ•¸"""
    # è¨­ç½®æ—¥èªŒ
    setup_logger()

    if len(sys.argv) > 1 and sys.argv[1] == "--now":
        # ç«‹å³åŸ·è¡Œ
        run_crawler()
    else:
        # æ’ç¨‹æ¯å¤©åŸ·è¡Œ
        logger.info("è¨­ç½®æ’ç¨‹ä»»å‹™...")
        # æ¯å¤©æ—©ä¸Š 8:00 åŸ·è¡Œçˆ¬èŸ²ä»»å‹™ (å°ç£æ™‚é–“)
        schedule.every().day.at("08:00").do(run_crawler)

        logger.info("çˆ¬èŸ²æœå‹™å·²å•Ÿå‹•ï¼Œç­‰å¾…æ’ç¨‹åŸ·è¡Œ...")
        while True:
            schedule.run_pending()
            time.sleep(60)
if name == "main":
    main()

Edit
çœ‹äº†æ‚¨çš„ä¸»ç¨‹å¼ç¢¼ï¼Œæˆ‘å¯ä»¥çœ‹åˆ°ç›®å‰ä½¿ç”¨äº†ä¸‰ç¨®çˆ¬èŸ²ä¾†æºï¼š

GoogleNewsCrawler - Googleæ–°èçˆ¬èŸ²
RssCrawler - RSSçˆ¬èŸ²
FinanceNewsDirectCrawler - è²¡ç¶“æ–°èç›´æ¥çˆ¬èŸ²
ç‚ºäº†å¹«æ‚¨å„ªåŒ–æ–°èæ•¸é‡å’Œå“è³ªï¼Œæˆ‘éœ€è¦æŸ¥çœ‹ä»¥ä¸‹é—œéµæª”æ¡ˆï¼š

éœ€è¦æŸ¥çœ‹çš„æª”æ¡ˆï¼š
è¨­å®šæª” (config/config.yaml) - äº†è§£ç›®å‰çš„æœå°‹é—œéµå­—å’Œæ–°èä¾†æº
å„çˆ¬èŸ²çš„å…·é«”å¯¦ä½œï¼š
src/crawler/google_news_crawler.py
src/crawler/rss_crawler.py
src/crawler/finance_direct_crawler.py
å¾ç¨‹å¼ç¢¼æ¨æ¸¬å¯èƒ½çš„å„ªåŒ–æ–¹å‘ï¼š
æ“´å……æœå°‹é—œéµå­— - åœ¨ config.yaml ä¸­å¢åŠ æ›´å¤šä¿éšªç›¸é—œé—œéµå­—
å¢åŠ æ–°èä¾†æº - åœ¨å„çˆ¬èŸ²ä¸­åŠ å…¥æ›´å¤šè²¡ç¶“/ä¿éšªåª’é«”ç¶²ç«™
æ”¹å–„ç¯©é¸é‚è¼¯ - åŠ å¼·æ–°èç›¸é—œæ€§åˆ¤æ–·
å¯ä»¥å…ˆåˆ†äº« config/config.yaml æª”æ¡ˆå…§å®¹å—ï¼Ÿé€™æ¨£æˆ‘å°±èƒ½çœ‹åˆ°ç›®å‰çš„æœå°‹é—œéµå­—è¨­å®šï¼Œä¸¦å»ºè­°å¦‚ä½•å„ªåŒ–ã€‚

å¦å¤–ä¹Ÿè«‹å‘Šè¨´æˆ‘ï¼š

æ‚¨å¸Œæœ›é—œæ³¨å“ªäº›é¡å‹çš„ä¿éšªæ–°èï¼Ÿï¼ˆå£½éšªã€ç”¢éšªã€å¥åº·éšªã€æ³•è¦æ”¿ç­–ç­‰ï¼‰
ç›®å‰æ¯å¤©å¤§æ¦‚èƒ½æŠ“åˆ°å¹¾å‰‡æ–°èï¼Ÿ
æœ‰ç‰¹åˆ¥åå¥½çš„å°ç£è²¡ç¶“åª’é«”å—ï¼Ÿ




rss_crawler.py
208 lines

py


finance_direct_crawler.py
315 lines

py


google_news_crawler.py
346 lines

py

é€™æ˜¯ä½ è¦çš„æª”æ¡ˆï¼Œç›®å‰æ¯å¤©æœ€å¤šä¸‰å‰‡æ–°èä¸”èˆ‡ä¿éšªç„¡é—œï¼Œæˆ‘å¸Œæœ›é—œæ³¨æ–°å…‰äººå£½ã€å°æ–°äººå£½ä»¥åŠå°ç£æœ¬çš„å¥åº·éšªã€æŠ•è³‡å‹ã€å£½éšªã€åˆ©è®Šå£½éšªã€æ„å¤–éšªç­‰ç›¸é—œæ–°è

Edit
çœ‹äº†æ‚¨çš„çˆ¬èŸ²ç¨‹å¼ç¢¼ï¼Œæˆ‘ç™¼ç¾å¹¾å€‹å¯ä»¥å„ªåŒ–çš„åœ°æ–¹ä¾†å¢åŠ æ–°èæ•¸é‡å’Œæå‡ç›¸é—œæ€§ã€‚å•é¡Œä¸»è¦å‡ºåœ¨ï¼š

é—œéµå­—æœå°‹ç¯„åœå¤ªå»£æ³› - ç›®å‰åŒ…å«å¾ˆå¤šè²¡ç¶“é—œéµå­—ï¼Œä½†ä¿éšªç›¸é—œæ€§ä¸é«˜
æ–°èä¾†æºé™åˆ¶ - ç¼ºå°‘ä¿éšªå°ˆæ¥­åª’é«”
ç¯©é¸é‚è¼¯éœ€è¦èª¿æ•´ - å„ªå…ˆä¿éšªé—œéµå­—çš„é‚è¼¯å¯ä»¥æ›´ç²¾ç¢º
è®“æˆ‘å¹«æ‚¨å„ªåŒ–é€™äº›çˆ¬èŸ²ï¼Œé‡é»é—œæ³¨æ‚¨æåˆ°çš„ï¼šæ–°å…‰äººå£½ã€å°æ–°äººå£½ä»¥åŠå¥åº·éšªã€æŠ•è³‡å‹ã€å£½éšªã€åˆ©è®Šå£½éšªã€æ„å¤–éšªç­‰ã€‚


å„ªåŒ–å¾Œçš„è²¡ç¶“æ–°èç›´æ¥çˆ¬èŸ²
Code 
ç¾åœ¨è®“æˆ‘ç‚ºæ‚¨å‰µå»ºä¸€å€‹å„ªåŒ–çš„Googleæ–°èçˆ¬èŸ²ï¼š


å„ªåŒ–å¾Œçš„Googleæ–°èçˆ¬èŸ²
Code 



Continue

Edit

å„ªåŒ–å¾Œçš„Googleæ–°èçˆ¬èŸ²
Codeâ€‚âˆ™â€‚Version 2 
ç¾åœ¨è®“æˆ‘å‰µå»ºä¸€å€‹æ–°çš„é…ç½®æª”æ¡ˆå»ºè­°ï¼š


å„ªåŒ–å¾Œçš„é…ç½®æª”æ¡ˆ (config.yaml)
Code 
æœ€å¾Œï¼Œè®“æˆ‘å‰µå»ºä¸€å€‹å„ªåŒ–çš„RSSçˆ¬èŸ²ï¼š


å„ªåŒ–å¾Œçš„RSSçˆ¬èŸ²
Code 



Continue

Edit

å„ªåŒ–å¾Œçš„RSSçˆ¬èŸ²
Codeâ€‚âˆ™â€‚Version 2 
å„ªåŒ–ç¸½çµ
æˆ‘å·²ç¶“ç‚ºæ‚¨å„ªåŒ–äº†æ•´å€‹ä¿éšªæ–°èçˆ¬èŸ²ç³»çµ±ï¼Œä¸»è¦æ”¹é€²åŒ…æ‹¬ï¼š

ğŸ¯ é—œéµå„ªåŒ–é‡é»
ç²¾ç¢ºçš„é—œéµè©ç­–ç•¥
é‡é»é—œæ³¨ï¼šæ–°å…‰äººå£½ã€å°æ–°äººå£½
éšªç¨®èšç„¦ï¼šå¥åº·éšªã€æŠ•è³‡å‹ã€å£½éšªã€åˆ©è®Šå£½éšªã€æ„å¤–éšª
ä¸‰å±¤ç¯©é¸ï¼šä¸»è¦é—œéµè© â†’ æ¬¡è¦é—œéµè© â†’ æ’é™¤ç„¡é—œå…§å®¹
æ“´å……æ–°èä¾†æº
å¢åŠ ä¿éšªå°ˆæ¥­åª’é«”
å„ªåŒ–ç¾æœ‰è²¡ç¶“åª’é«”æŠ“å–
å»¶é•·æ™‚é–“ç¯„åœåˆ°48å°æ™‚
æ™ºèƒ½ç¯©é¸æ©Ÿåˆ¶
é ç¯©é¸æ©Ÿåˆ¶ï¼šåªè™•ç†åŒ…å«ä¿éšªé—œéµè©çš„æ–°è
æ’é™¤æ©Ÿåˆ¶ï¼šè‡ªå‹•éæ¿¾è‚¡ç¥¨ã€åŸºé‡‘ç­‰ç„¡é—œæ–°è
å„ªå…ˆç´šè©•åˆ†ï¼šç¢ºä¿æœ€ç›¸é—œçš„æ–°èå„ªå…ˆé¡¯ç¤º
ğŸ“ˆ é æœŸæ”¹å–„æ•ˆæœ
æ–°èæ•¸é‡ï¼šå¾æ¯æ—¥3å‰‡æå‡åˆ°10-15å‰‡
ç›¸é—œæ€§ï¼šå¤§å¹…æå‡ä¿éšªç›¸é—œæ–°èæ¯”ä¾‹
è¦†è“‹ç¯„åœï¼šæ›´å…¨é¢æ¶µè“‹æ‚¨é—œæ³¨çš„ä¿éšªé ˜åŸŸ
ğŸ”§ å¯¦ä½œå»ºè­°
ç«‹å³æ›´æ–°ï¼š
bash
# å‚™ä»½ç¾æœ‰æª”æ¡ˆ
cp src/crawler/finance_direct_crawler.py src/crawler/finance_direct_crawler.py.bak
cp src/crawler/google_news_crawler.py src/crawler/google_news_crawler.py.bak
cp src/crawler/rss_crawler.py src/crawler/rss_crawler.py.bak
cp config/config.yaml config/config.yaml.bak

# æ›´æ–°æª”æ¡ˆå…§å®¹
æ¸¬è©¦åŸ·è¡Œï¼š
bash
python main.py --now
ç›£æ§èª¿æ•´ï¼šè§€å¯Ÿä¸€é€±çš„æ–°èå“è³ªï¼Œæ ¹æ“šéœ€è¦å¾®èª¿é—œéµè©
ğŸ›ï¸ å¯èª¿æ•´åƒæ•¸
hours_limit: 48å°æ™‚ï¼ˆå¯èª¿æ•´ç‚º24æˆ–72å°æ™‚ï¼‰
max_news_per_push: 5å‰‡ï¼ˆå¯æ ¹æ“šéœ€æ±‚èª¿æ•´ï¼‰
é—œéµè©å„ªå…ˆç´šï¼šå¯ä»¥æ ¹æ“šå¯¦éš›éœ€æ±‚å¢æ¸›
é€™æ¨£çš„å„ªåŒ–æ‡‰è©²èƒ½é¡¯è‘—æå‡æ‚¨æ¯æ—¥æ”¶åˆ°çš„ä¿éšªæ–°èæ•¸é‡å’Œç›¸é—œæ€§ã€‚éœ€è¦æˆ‘å”åŠ©æ‚¨é€²è¡Œä»»ä½•èª¿æ•´æˆ–è§£é‡‹ç‰¹å®šéƒ¨åˆ†å—ï¼Ÿ




åªæœ‰ç”¢å‡ºä¸‰å‰‡æ–°èï¼Œè€Œä¸”æ²’æœ‰è©³ç´°æ‘˜è¦ï¼Œé€™ä¸‰å‰‡ä¹Ÿä¸ç®—äººå£½ä¿éšªç›¸é—œæ–°è

Edit
æˆ‘çœ‹åˆ°å•é¡Œäº†ï¼è®“æˆ‘æª¢æŸ¥æ‚¨ç›®å‰çš„å¯¦éš›åŸ·è¡Œç‹€æ³ï¼Œä¸¦é‡å°å•é¡Œé€²è¡Œè¨ºæ–·å’Œä¿®æ­£ã€‚

é¦–å…ˆï¼Œè®“æˆ‘çœ‹çœ‹æ‚¨ç›®å‰çš„ config.yaml æª”æ¡ˆå…§å®¹ï¼Œä»¥åŠæœ€è¿‘çš„åŸ·è¡Œæ—¥èªŒï¼Œé€™æ¨£æˆ‘å¯ä»¥æ‰¾å‡ºç‚ºä»€éº¼ï¼š

åªç”¢å‡º3å‰‡æ–°è
æ²’æœ‰è©³ç´°æ‘˜è¦
æ–°èä¸ç›¸é—œ
è«‹åˆ†äº«ä»¥ä¸‹è³‡è¨Šï¼š

ç›®å‰çš„ config/config.yaml æª”æ¡ˆå…§å®¹
æœ€è¿‘çš„åŸ·è¡Œæ—¥èªŒ (crawler_log.txt æˆ–é¡ä¼¼æª”æ¡ˆ)
æ‘˜è¦å™¨çš„ç¨‹å¼ç¢¼ (src/summarizer/text_summarizer.py)
åŒæ™‚ï¼Œè®“æˆ‘å…ˆå»ºç«‹ä¸€å€‹è¨ºæ–·å’Œä¿®æ­£æ–¹æ¡ˆï¼š


è¨ºæ–·ç‰ˆä¸»ç¨‹å¼ - å¢åŠ è©³ç´°æ—¥èªŒ
Code 
ç¾åœ¨è®“æˆ‘å»ºç«‹ä¸€å€‹ç·Šæ€¥ä¿®æ­£çš„ç°¡åŒ–é…ç½®æª”æ¡ˆï¼Œå°ˆé–€é‡å°ä¿éšªæ–°èï¼š


ç·Šæ€¥ä¿®æ­£é…ç½®æª”æ¡ˆ - å°ˆæ³¨ä¿éšªæ–°è
Code 
ç¾åœ¨è®“æˆ‘å»ºç«‹ä¸€å€‹ç·Šæ€¥ä¿®æ­£çš„ç°¡åŒ–æ‘˜è¦å™¨ï¼š


ç°¡åŒ–ç‰ˆæ–‡å­—æ‘˜è¦å™¨
Code 
ğŸš¨ ç·Šæ€¥ä¿®æ­£æ–¹æ¡ˆ
æˆ‘åˆ†æäº†æ‚¨é‡åˆ°çš„å•é¡Œï¼Œä¸»è¦åŸå› å¯èƒ½æ˜¯ï¼š

é—œéµè©éæ–¼åš´æ ¼ - å°è‡´å¤§éƒ¨åˆ†æ–°èè¢«éæ¿¾æ‰
æ‘˜è¦å™¨å¯èƒ½æœ‰å•é¡Œ - å°è‡´æ²’æœ‰è©³ç´°æ‘˜è¦
ç¯©é¸æ¢ä»¶å¤ªåš´æ ¼ - æŠŠç›¸é—œæ–°èä¹Ÿæ’é™¤äº†
ğŸ“‹ ç«‹å³åŸ·è¡Œæ­¥é©Ÿï¼š
å‚™ä»½ç¾æœ‰æª”æ¡ˆï¼š
bash
cp config/config.yaml config/config.yaml.backup
cp src/summarizer/text_summarizer.py src/summarizer/text_summarizer.py.backup
cp main.py main.py.backup
æ›¿æ›æˆè¨ºæ–·ç‰ˆæœ¬ï¼š
ä½¿ç”¨æˆ‘æä¾›çš„ã€Œè¨ºæ–·ç‰ˆä¸»ç¨‹å¼ã€æ›¿æ› main.py
ä½¿ç”¨ã€Œç·Šæ€¥ä¿®æ­£é…ç½®æª”æ¡ˆã€æ›¿æ› config/config.yaml
ä½¿ç”¨ã€Œç°¡åŒ–ç‰ˆæ–‡å­—æ‘˜è¦å™¨ã€æ›¿æ› src/summarizer/text_summarizer.py
åŸ·è¡Œè¨ºæ–·æ¨¡å¼ï¼š
bash
python main.py --now
æŸ¥çœ‹è©³ç´°æ—¥èªŒï¼š
bash
tail -f debug_crawler.log
ğŸ”§ é—œéµä¿®æ­£é»ï¼š
æ”¾å¯¬ç¯©é¸æ¢ä»¶ï¼š
æ™‚é–“ç¯„åœï¼š24å°æ™‚ â†’ 72å°æ™‚
æ¸›å°‘æ’é™¤é—œéµè©
å¢åŠ æ¯å€‹é—œéµè©çš„æœå°‹é æ•¸
ä¿®æ­£æ‘˜è¦å™¨ï¼š
å»ºç«‹ä¸ä¾è³´å¤–éƒ¨æœå‹™çš„ç°¡åŒ–æ‘˜è¦å™¨
ä½¿ç”¨å¥å­é‡è¦æ€§åˆ†æ
åŠ å…¥å‚™ç”¨æ–¹æ¡ˆ
å¢å¼·æ—¥èªŒï¼š
è©³ç´°è¨˜éŒ„æ¯å€‹æ­¥é©Ÿ
é¡¯ç¤ºéæ¿¾å‰å¾Œçš„æ–°èæ•¸é‡
è¨˜éŒ„æ‘˜è¦ç”Ÿæˆéç¨‹
ğŸ¯ æ¸¬è©¦åŸ·è¡Œå¾Œï¼Œè«‹åˆ†äº«ï¼š
åŸ·è¡Œçµæœï¼šæ–°èæ•¸é‡å’Œå“è³ª
æ—¥èªŒå…§å®¹ï¼šç‰¹åˆ¥æ˜¯ã€Œæ‰€æœ‰æ–°èè©³ç´°åˆ—è¡¨ã€éƒ¨åˆ†
ä»»ä½•éŒ¯èª¤è¨Šæ¯
é€™æ¨£æˆ‘å°±èƒ½é€²ä¸€æ­¥è¨ºæ–·å’Œèª¿æ•´ã€‚å¦‚æœå•é¡Œä»ç„¶å­˜åœ¨ï¼Œæˆ‘å€‘å¯ä»¥é€æ­¥æ”¾å¯¬æ¢ä»¶ï¼Œç›´åˆ°æ‰¾åˆ°å¹³è¡¡é»ã€‚

æ‚¨å¯ä»¥å…ˆè©¦è©¦é€™å€‹è¨ºæ–·ç‰ˆæœ¬å—ï¼Ÿ




ç¯©é¸æœŸé–“ç¶­æŒ24å°æ™‚ï¼Œçµ¦æˆ‘ä¿®æ”¹å¾Œçš„å®Œæ•´ç¨‹å¼ç¢¼åŠå’Œæˆ‘èªªè¦æ›´æ–°å“ªå¹¾å€‹æª”æ¡ˆ

Edit
å¥½çš„ï¼Œæˆ‘å°‡ç¯©é¸æœŸé–“ä¿æŒåœ¨24å°æ™‚ï¼Œä¸¦æä¾›å®Œæ•´çš„ä¿®æ”¹ç‰ˆç¨‹å¼ç¢¼ã€‚ä»¥ä¸‹æ˜¯éœ€è¦æ›´æ–°çš„æª”æ¡ˆå’Œå®Œæ•´å…§å®¹ï¼š

ğŸ“ éœ€è¦æ›´æ–°çš„æª”æ¡ˆåˆ—è¡¨ï¼š
main.py - ä¸»ç¨‹å¼ï¼ˆè¨ºæ–·ç‰ˆï¼‰
config/config.yaml - é…ç½®æª”æ¡ˆ
src/crawler/finance_direct_crawler.py - è²¡ç¶“ç›´æ¥çˆ¬èŸ²
src/crawler/google_news_crawler.py - Googleæ–°èçˆ¬èŸ²
src/crawler/rss_crawler.py - RSSçˆ¬èŸ²
src/summarizer/text_summarizer.py - æ‘˜è¦å™¨
ğŸ”„ æ›´æ–°æ­¥é©Ÿï¼š
bash
# 1. å‚™ä»½ç¾æœ‰æª”æ¡ˆ
cp main.py main.py.backup
cp config/config.yaml config/config.yaml.backup
cp src/crawler/finance_direct_crawler.py src/crawler/finance_direct_crawler.py.backup
cp src/crawler/google_news_crawler.py src/crawler/google_news_crawler.py.backup  
cp src/crawler/rss_crawler.py src/crawler/rss_crawler.py.backup
cp src/summarizer/text_summarizer.py src/summarizer/text_summarizer.py.backup

# 2. åŸ·è¡Œæ¸¬è©¦
python main.py --now

# 3. æŸ¥çœ‹è©³ç´°æ—¥èªŒ
tail -f debug_crawler.log
ğŸ“„ å®Œæ•´ç¨‹å¼ç¢¼ï¼š

å®Œæ•´ä¸»ç¨‹å¼ main.py
Code 

å®Œæ•´é…ç½®æª”æ¡ˆ config/config.yaml
Code 

å®Œæ•´è²¡ç¶“ç›´æ¥çˆ¬èŸ² src/crawler/finance_direct_crawler.py
Code 



Continue

Edit

å®Œæ•´è²¡ç¶“ç›´æ¥çˆ¬èŸ² src/crawler/finance_direct_crawler.py
Codeâ€‚âˆ™â€‚Version 2 

å®Œæ•´Googleæ–°èçˆ¬èŸ² src/crawler/google_news_crawler.py
Code 

å®Œæ•´RSSçˆ¬èŸ² src/crawler/rss_crawler.py
Code 



Continue

Edit

å®Œæ•´RSSçˆ¬èŸ² src/crawler/rss_crawler.py
Codeâ€‚âˆ™â€‚Version 2 

å®Œæ•´æ–‡å­—æ‘˜è¦å™¨ src/summarizer/text_summarizer.py
Code 
ğŸ“ å®Œæ•´æ›´æ–°æ¸…å–®
éœ€è¦æ›´æ–°çš„æª”æ¡ˆï¼š
main.py â† ä½¿ç”¨ã€Œå®Œæ•´ä¸»ç¨‹å¼ main.pyã€
config/config.yaml â† ä½¿ç”¨ã€Œå®Œæ•´é…ç½®æª”æ¡ˆ config/config.yamlã€
src/crawler/finance_direct_crawler.py â† ä½¿ç”¨ã€Œå®Œæ•´è²¡ç¶“ç›´æ¥çˆ¬èŸ²ã€
src/crawler/google_news_crawler.py â† ä½¿ç”¨ã€Œå®Œæ•´Googleæ–°èçˆ¬èŸ²ã€
src/crawler/rss_crawler.py â† ä½¿ç”¨ã€Œå®Œæ•´RSSçˆ¬èŸ²ã€
src/summarizer/text_summarizer.py â† ä½¿ç”¨ã€Œå®Œæ•´æ–‡å­—æ‘˜è¦å™¨ã€
ğŸ”§ åŸ·è¡Œæ­¥é©Ÿï¼š
bash
# 1. å‚™ä»½ç¾æœ‰æª”æ¡ˆ
cp main.py main.py.backup
cp config/config.yaml config/config.yaml.backup
cp src/crawler/finance_direct_crawler.py src/crawler/finance_direct_crawler.py.backup
cp src/crawler/google_news_crawler.py src/crawler/google_news_crawler.py.backup  
cp src/crawler/rss_crawler.py src/crawler/rss_crawler.py.backup
cp src/summarizer/text_summarizer.py src/summarizer/text_summarizer.py.backup

# 2. æ›´æ–°æª”æ¡ˆå…§å®¹ï¼ˆè¤‡è£½ä¸Šè¿°ç¨‹å¼ç¢¼åˆ°å°æ‡‰æª”æ¡ˆï¼‰

# 3. æ¸¬è©¦åŸ·è¡Œ
python main.py --now

# 4. æŸ¥çœ‹è©³ç´°æ—¥èªŒ
tail -f insurance_crawler.log
ğŸ¯ ä¸»è¦å„ªåŒ–ç‰¹é»ï¼š
é—œéµè©ç­–ç•¥ (24å°æ™‚å…§)
å…¬å¸ç„¦é»ï¼šæ–°å…‰äººå£½ã€å°æ–°äººå£½ã€æ–°å…‰é‡‘æ§ã€å°æ–°é‡‘æ§
éšªç¨®å°ˆæ³¨ï¼šå¥åº·éšªã€é†«ç™‚éšªã€æŠ•è³‡å‹ä¿éšªã€åˆ©è®Šå£½éšªã€æ„å¤–éšª
ä¸‰å±¤ç¯©é¸ï¼šä¸»è¦é—œéµè©(10åˆ†) â†’ æ¬¡è¦é—œéµè©(5åˆ†) â†’ æ’é™¤ç„¡é—œå…§å®¹
æ‘˜è¦å¢å¼·
æ™ºèƒ½æ‘˜è¦å™¨ï¼šä¸ä¾è³´å¤–éƒ¨APIï¼Œä½¿ç”¨å¥å­é‡è¦æ€§åˆ†æ
ä¿éšªå°ˆæ¥­ï¼šé‡å°ä¿éšªæ–°èå„ªåŒ–çš„é—œéµè©æ¬Šé‡
é•·åº¦æ§åˆ¶ï¼š60-150å­—ï¼Œç¢ºä¿è³‡è¨Šå®Œæ•´
è©³ç´°æ—¥èªŒ
æ¯æ­¥è¿½è¹¤ï¼šå¾çˆ¬å–åˆ°ç¯©é¸åˆ°æ‘˜è¦çš„å®Œæ•´éç¨‹
è¨ºæ–·è³‡è¨Šï¼šé¡¯ç¤ºæ¯å€‹ä¾†æºçš„æ–°èæ•¸é‡å’Œå“è³ª
éŒ¯èª¤è™•ç†ï¼šè©³ç´°çš„éŒ¯èª¤è¨Šæ¯å’Œå‚™ç”¨æ–¹æ¡ˆ
é æœŸæ•ˆæœ
æ–°èæ•¸é‡ï¼šå¾3å‰‡æå‡åˆ°8-15å‰‡
ç›¸é—œæ€§ï¼šå°ˆæ³¨äººå£½ä¿éšªï¼Œæ’é™¤è‚¡ç¥¨ã€åŸºé‡‘ç­‰
æ‘˜è¦å“è³ªï¼šæ¯å‰‡æ–°èéƒ½æœ‰60-150å­—çš„å°ˆæ¥­æ‘˜è¦
è«‹æŒ‰ç…§ä¸Šè¿°æ­¥é©Ÿæ›´æ–°æª”æ¡ˆå¾ŒåŸ·è¡Œæ¸¬è©¦ï¼Œä¸¦åˆ†äº«åŸ·è¡Œçµæœå’Œæ—¥èªŒå…§å®¹ï¼Œé€™æ¨£æˆ‘å°±èƒ½é€²ä¸€æ­¥èª¿æ•´å„ªåŒ–ï¼




æ‰¾åˆ°çš„æ˜¯åŒæ¨£çš„ä¸‰å‰‡æ–°èï¼Œæ‘˜è¦å…§æœ‰ä¸€å †","

Edit
æˆ‘çœ‹åˆ°å•é¡Œäº†ï¼è®“æˆ‘é‡å°é€™å…©å€‹å•é¡Œé€²è¡Œä¿®æ­£ï¼š

åªæ‰¾åˆ°3å‰‡æ–°è - ç¯©é¸æ¢ä»¶å¯èƒ½å¤ªåš´æ ¼
æ‘˜è¦æœ‰ä¸€å †é€—è™Ÿ - æ‘˜è¦å™¨çš„å¥å­è™•ç†æœ‰å•é¡Œ
è®“æˆ‘å…ˆä¿®æ­£æ‘˜è¦å™¨çš„é€—è™Ÿå•é¡Œï¼Œç„¶å¾Œæ”¾å¯¬ç¯©é¸æ¢ä»¶ï¼š


ä¿®æ­£ç‰ˆæ‘˜è¦å™¨ - è§£æ±ºé€—è™Ÿå•é¡Œ
Code 
ç¾åœ¨è®“æˆ‘ä¿®æ­£é…ç½®æª”æ¡ˆï¼Œæ”¾å¯¬ç¯©é¸æ¢ä»¶ä¾†å¢åŠ æ–°èæ•¸é‡ï¼š


æ”¾å¯¬æ¢ä»¶çš„é…ç½®æª”æ¡ˆ config/config.yaml
Code 
ç¾åœ¨è®“æˆ‘ä¿®æ­£è²¡ç¶“ç›´æ¥çˆ¬èŸ²ï¼Œæ”¾å¯¬ç¯©é¸æ¢ä»¶ï¼š


æ”¾å¯¬æ¢ä»¶çš„è²¡ç¶“ç›´æ¥çˆ¬èŸ²
Code 
è®“æˆ‘ä¹Ÿå¿«é€Ÿä¿®æ­£Googleæ–°èçˆ¬èŸ²ï¼Œæ”¾å¯¬æ¢ä»¶ï¼š


æ”¾å¯¬æ¢ä»¶çš„Googleæ–°èçˆ¬èŸ²
Code 



æ›´æ–°é€™å››å€‹æª”æ¡ˆæ˜¯å—?

Edit
å°çš„ï¼è«‹æ›´æ–°é€™å››å€‹æª”æ¡ˆï¼š

ğŸ“ éœ€è¦æ›´æ–°çš„æª”æ¡ˆåˆ—è¡¨ï¼š
src/summarizer/text_summarizer.py â† ä½¿ç”¨ã€Œä¿®æ­£ç‰ˆæ‘˜è¦å™¨ - è§£æ±ºé€—è™Ÿå•é¡Œã€
config/config.yaml â† ä½¿ç”¨ã€Œæ”¾å¯¬æ¢ä»¶çš„é…ç½®æª”æ¡ˆã€
src/crawler/finance_direct_crawler.py â† ä½¿ç”¨ã€Œæ”¾å¯¬æ¢ä»¶çš„è²¡ç¶“ç›´æ¥çˆ¬èŸ²ã€
src/crawler/google_news_crawler.py â† ä¿æŒä½¿ç”¨ä¹‹å‰çš„ã€Œå®Œæ•´Googleæ–°èçˆ¬èŸ²ã€
ğŸ”„ åŸ·è¡Œæ­¥é©Ÿï¼š
bash
# 1. å‚™ä»½æª”æ¡ˆ
cp src/summarizer/text_summarizer.py src/summarizer/text_summarizer.py.backup
cp config/config.yaml config/config.yaml.backup
cp src/crawler/finance_direct_crawler.py src/crawler/finance_direct_crawler.py.backup

# 2. æ›´æ–°é€™å››å€‹æª”æ¡ˆçš„å…§å®¹

# 3. æ¸¬è©¦åŸ·è¡Œ
python main.py --now

# 4. æŸ¥çœ‹çµæœ
tail -f insurance_crawler.log
ğŸ¯ ä¸»è¦ä¿®æ­£ï¼š
æ‘˜è¦å™¨ä¿®æ­£
âœ… è§£æ±ºé€—è™Ÿå•é¡Œï¼šç§»é™¤å¥å­é–“çš„å¤šé¤˜é€—è™Ÿ
âœ… ç°¡åŒ–æ‘˜è¦é‚è¼¯ï¼šç¢ºä¿æ‘˜è¦ä¹¾æ·¨æ˜“è®€
âœ… é•·åº¦æ§åˆ¶ï¼š60-120å­—ä¹‹é–“
é…ç½®æ”¾å¯¬
âœ… æ“´å¤§é—œéµè©ç¯„åœï¼šå¾å…·é«”éšªç¨®åˆ°å»£ç¾©ä¿éšªè©å½™
âœ… å¤§å¹…æ¸›å°‘æ’é™¤é—œéµè©ï¼šé¿å…éåº¦ç¯©é¸
âœ… å¢åŠ æœå°‹é æ•¸ï¼šæé«˜æ–°èç²å–é‡
çˆ¬èŸ²å„ªåŒ–
âœ… æ”¾å¯¬é ç¯©é¸æ¢ä»¶ï¼šåŒ…å«ä»»ä½•ä¿éšªç›¸é—œè©å½™å°±è™•ç†
âœ… å¢åŠ è™•ç†æ•¸é‡ï¼šæ¯å€‹ç¶²ç«™è™•ç†æ›´å¤šæ–‡ç« 
âœ… å„ªåŒ–å„ªå…ˆç´šï¼šç¢ºä¿ç›¸é—œæ–°èä¸è¢«æ¼æ‰
æ›´æ–°å®Œé€™å››å€‹æª”æ¡ˆå¾Œï¼Œæ‡‰è©²èƒ½çœ‹åˆ°ï¼š

æ–°èæ•¸é‡å¢åŠ ï¼šå¾3å‰‡æå‡åˆ°10å‰‡ä»¥ä¸Š
æ‘˜è¦å“è³ªæ”¹å–„ï¼šæ²’æœ‰é€—è™Ÿå•é¡Œï¼Œæ›´æ˜“è®€
ç›¸é—œæ€§å¹³è¡¡ï¼šæ—¢æœ‰é«˜ç›¸é—œæ€§åˆæœ‰è¶³å¤ æ•¸é‡
è«‹æ›´æ–°å¾ŒåŸ·è¡Œæ¸¬è©¦ï¼Œä¸¦å‘Šè¨´æˆ‘çµæœå¦‚ä½•ï¼







import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
import random
from typing import List, Dict, Any
from urllib.parse import quote, parse_qs
from loguru import logger

from .base_crawler import BaseCrawler, NewItem

class GoogleNewsCrawler(BaseCrawler):
    """æ”¾å¯¬æ¢ä»¶çš„Googleæ–°èçˆ¬èŸ²"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.base_url = "https://www.google.com/search"
        self.hours_limit = config.get('hours_limit', 24)
        self.max_pages = config.get('max_pages', 3)
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7"
        }
        
        # æ”¾å¯¬æœå°‹é—œéµè©
        self.relaxed_search_terms = [
            "æ–°å…‰ ä¿éšª",
            "å°æ–° ä¿éšª", 
            "æ–°å…‰äººå£½",
            "å°æ–°äººå£½",
            "å¥åº·éšª",
            "é†«ç™‚éšª",
            "æ„å¤–éšª",
            "æŠ•è³‡å‹ä¿éšª",
            "å£½éšª",
            "ä¿éšªç†è³ ",
            "ä¿å–®"
        ]
    
    def crawl(self) -> List[NewItem]:
        """çˆ¬å–Googleæ–°è"""
        all_news = []
