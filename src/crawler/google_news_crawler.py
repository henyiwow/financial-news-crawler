import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
import random
from typing import List, Dict, Any
from urllib.parse import quote, parse_qs
from loguru import logger

from .base_crawler import BaseCrawler, NewItem

class GoogleNewsCrawler(BaseCrawler):
    """å„ªåŒ–å¾Œçš„Googleæ–°èçˆ¬èŸ² - å°ˆæ³¨ä¿éšªæ–°è"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.base_url = "https://www.google.com/search"
        self.region = config.get('region', 'tw')
        self.hours_limit = config.get('hours_limit', 24)
        self.max_pages = config.get('max_pages', 3)
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7"
        }
        
        # å°ˆé–€é‡å°ä¿éšªçš„æœå°‹é—œéµè©
        self.insurance_search_terms = [
            # å…¬å¸åç¨±ç›¸é—œ
            "æ–°å…‰äººå£½",
            "å°æ–°äººå£½", 
            "æ–°å…‰é‡‘æ§",
            "å°æ–°é‡‘æ§",
            
            # éšªç¨®ç›¸é—œ
            "å¥åº·éšª å°ç£",
            "é†«ç™‚éšª ä¿éšª",
            "æŠ•è³‡å‹ä¿éšª",
            "åˆ©è®Šå£½éšª",
            "æ„å¤–éšª ç†è³ ",
            "å¹´é‡‘éšª",
            "å„²è“„éšª",
            
            # ä¿éšªæ¥­å‹™
            "ä¿éšªç†è³ ",
            "ä¿å–®çµ¦ä»˜",
            "ä¿éšªæ–°å•†å“",
            "ä¿éšªæ³•è¦ å°ç£"
        ]
        
        # å„ªå…ˆé—œéµè©
        self.priority_keywords = [
            "æ–°å…‰äººå£½", "æ–°å…‰é‡‘æ§", "å°æ–°äººå£½", "å°æ–°é‡‘æ§",
            "å¥åº·éšª", "æŠ•è³‡å‹ä¿éšª", "åˆ©è®Šå£½éšª", "æ„å¤–éšª"
        ]
    
    def crawl(self) -> List[NewItem]:
        """çˆ¬å–Googleæ–°è"""
        all_news = []
        
        # åˆä½µå°ˆé–€çš„ä¿éšªæœå°‹é—œéµè©å’ŒåŸå§‹é—œéµè©
        search_terms = self.insurance_search_terms + self.search_terms
        
        for term in search_terms:
            logger.info(f"ğŸ” Googleæœå°‹é—œéµè©: {term}")
            try:
                news_items = self._search_term_multiple_pages(term)
                all_news.extend(news_items)
                
                # é¿å…è¢«Googleå°é–
                time.sleep(random.uniform(3, 6))
            except Exception as e:
                logger.error(f"âŒ çˆ¬å–é—œéµè© '{term}' æ™‚å‡ºéŒ¯: {str(e)}")
        
        # å»é‡è¤‡
        unique_news = self._remove_duplicates(all_news)
        logger.info(f"ğŸ”„ å»é‡å¾Œå‰©é¤˜ {len(unique_news)} æ¢æ–°è")
        
        # éæ¿¾ä¿éšªç›¸é—œæ–°è
        filtered_news = self._filter_insurance_news(unique_news)
        logger.info(f"ğŸ¯ éæ¿¾å¾Œå‰©é¤˜ {len(filtered_news)} æ¢ä¿éšªç›¸é—œæ–°è")
        
        # æ ¹æ“šå„ªå…ˆé †åºæ’åº
        sorted_news = self.sort_by_priority(filtered_news)
        return sorted_news[:15]  # è¿”å›å‰15æ¢
    
    def _filter_insurance_news(self, news_list: List[NewItem]) -> List[NewItem]:
        """éæ¿¾å‡ºä¿éšªç›¸é—œæ–°è"""
        filtered_news = []
        
        # ä¿éšªç›¸é—œé—œéµè©
        insurance_keywords = [
            "æ–°å…‰äººå£½", "æ–°å…‰é‡‘æ§", "å°æ–°äººå£½", "å°æ–°é‡‘æ§",
            "ä¿éšª", "å£½éšª", "å¥åº·éšª", "é†«ç™‚éšª", "æ„å¤–éšª", "å‚·å®³éšª",
            "æŠ•è³‡å‹ä¿éšª", "è®Šé¡ä¿éšª", "åˆ©è®Šå£½éšª", "å¹´é‡‘éšª", "å„²è“„éšª",
            "ç†è³ ", "çµ¦ä»˜", "ä¿å–®", "ä¿è²»", "æ‰¿ä¿", "æ ¸ä¿",
            "é‡å¤§ç–¾ç—…éšª", "ç™Œç—‡éšª", "å¯¦æ”¯å¯¦ä»˜"
        ]
        
        # æ’é™¤é—œéµè©
        exclude_keywords = [
            "è‚¡åƒ¹", "é…æ¯", "é™¤æ¬Š", "é™¤æ¯", "ETF", "åŸºé‡‘", "å‚µåˆ¸",
            "éŠ€è¡Œå­˜æ¬¾", "ä¿¡ç”¨å¡", "æˆ¿è²¸", "è»Šè²¸"
        ]
        
        for item in news_list:
            title_content = (item.title + " " + (item.content or "")).lower()
            
            # æª¢æŸ¥æ˜¯å¦åŒ…å«æ’é™¤é—œéµè©
            contains_exclude = any(exclude_word in title_content for exclude_word in exclude_keywords)
            if contains_exclude:
                continue
            
            # æª¢æŸ¥æ˜¯å¦åŒ…å«ä¿éšªé—œéµè©
            contains_insurance = any(keyword in title_content for keyword in insurance_keywords)
            
            if contains_insurance:
                # è¨ˆç®—å„ªå…ˆç´šåˆ†æ•¸
                priority_score = 0
                matched_keyword = ""
                
                for keyword in self.priority_keywords:
                    if keyword in title_content:
                        priority_score += 10
                        matched_keyword = keyword
                        break
                
                for keyword in insurance_keywords:
                    if keyword in title_content:
                        if priority_score == 0:
                            priority_score = 5
                            matched_keyword = keyword
                        break
                
                item.keyword = matched_keyword
                item.priority_score = priority_score
                filtered_news.append(item)
                logger.debug(f"âœ… ä¿éšªç›¸é—œæ–°è: {item.title[:30]}...")
        
        return filtered_news
    
    def _search_term_multiple_pages(self, term: str) -> List[NewItem]:
        """æœå°‹å¤šé çµæœ"""
        all_news = []
        
        for page in range(self.max_pages):
            logger.debug(f"ğŸ” æœå°‹é—œéµè© '{term}' ç¬¬ {page + 1} é ")
            
            try:
                news_items = self._search_term(term, page)
                all_news.extend(news_items)
                
                # å¦‚æœæ²’æœ‰æ‰¾åˆ°æ–°èï¼Œæå‰çµæŸ
                if not news_items:
                    logger.debug(f"âš ï¸ é—œéµè© '{term}' ç¬¬ {page + 1} é ç„¡çµæœï¼Œåœæ­¢æœå°‹")
                    break
                
                # é é¢é–“å»¶é²
                if page < self.max_pages - 1:
                    time.sleep(random.uniform(2, 4))
                    
            except Exception as e:
                logger.error(f"âŒ æœå°‹é—œéµè© '{term}' ç¬¬ {page + 1} é æ™‚å‡ºéŒ¯: {str(e)}")
                break
        
        logger.debug(f"ğŸ“Š é—œéµè© '{term}' ç¸½å…±æ‰¾åˆ° {len(all_news)} æ¢æ–°è")
        return all_news
    
    def _search_term(self, term: str, page: int = 0) -> List[NewItem]:
        """ä½¿ç”¨ç‰¹å®šé—œéµè©æœå°‹Googleæ–°è"""
        news_items = []
        
        # æ§‹å»ºæ›´ç²¾ç¢ºçš„æŸ¥è©¢åƒæ•¸
        params = {
            "q": f"{term} site:tw OR site:com.tw",  # é™åˆ¶å°ç£ç¶²ç«™
            "tbm": "nws",  # æ–°èæœå°‹
            "tbs": "qdr:d",  # æœ€è¿‘1å¤©
            "hl": "zh-TW",  # èªè¨€
            "gl": "tw",     # åœ°å€ï¼šå°ç£
            "start": page * 10,  # åˆ†é åƒæ•¸
            "num": 20  # æ¯é çµæœæ•¸
        }
        
        try:
            response = requests.get(self.base_url, params=params, headers=self.headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å˜—è©¦å¤šç¨®å¯èƒ½çš„æ–°èå…ƒç´ é¸æ“‡å™¨
            news_divs = []
            
            selector_attempts = [
                "div.SoaBEf",
                "div.WlydOe", 
                "div.xuvV6b",
                "div.DBQmFf",
                "g-card.ftSUBd",
                "div.v7W49e",
                "div.g",
                ".g .rc",
                "[jscontroller='SC7lYd']"
            ]
            
            for selector in selector_attempts:
                elements = soup.select(selector)
                if elements:
                    news_divs = elements
                    logger.debug(f"ğŸ” ç¬¬ {page + 1} é æ‰¾åˆ°é¸æ“‡å™¨ {selector} çš„æ–°èå…ƒç´ : {len(elements)} å€‹")
                    break
            
            if not news_divs:
                # æ›´é€šç”¨çš„æ–¹æ³•
                news_divs = soup.find_all("div", recursive=True, limit=50)
                news_divs = [div for div in news_divs if div.find("a") and div.find("h3")]
                logger.debug(f"ğŸ” ç¬¬ {page + 1} é ä½¿ç”¨é€šç”¨æ–¹æ³•æ‰¾åˆ° {len(news_divs)} å€‹å¯èƒ½çš„æ–°èå…ƒç´ ")
            
            count = 0
            for div in news_divs:
                if count >= 20:  # æ¯é è™•ç†20æ¢æ–°è
                    break
                
                try:
                    # è§£ææ–°èå…ƒç´ 
                    title = None
                    url = None
                    source = None
                    time_text = None
                    
                    # å°‹æ‰¾æ¨™é¡Œ
                    title_elements = [
                        div.find("div", class_="mCBkyc"),
                        div.find("h3"),
                        div.find("a", class_="DY5T1d"),
                        div.find(["h3", "h4", "h2"]),
                        div.find("div", class_="BNeawe"),
                        div.find("div", class_="r")
                    ]
                    
                    for element in title_elements:
                        if element and element.get_text().strip():
                            title = element.get_text().strip()
                            break
                    
                    # å°‹æ‰¾é€£çµ
                    link_element = div.find("a")
                    if link_element and link_element.get("href"):
                        url = link_element.get("href")
                        if url.startswith("/url?"):
                            try:
                                parsed = parse_qs(url.split("?")[1])
                                if "url" in parsed and parsed["url"]:
                                    url = parsed["url"][0]
                            except Exception as e:
                                logger.warning(f"âš ï¸ è§£æURLæ™‚å‡ºéŒ¯: {str(e)}")
                    
                    # å°‹æ‰¾ä¾†æº
                    source_elements = [
                        div.find("div", class_="CEMjEf"),
                        div.find("div", class_="UPmit"),
                        div.find("span", class_="xQ82C"),
                        div.find("div", class_="BNeawe"),
                        div.find("cite"),
                        div.find(["div", "span"], string=lambda s: "Â·" in s if s else False),
                    ]
                    
                    for element in source_elements:
                        if element and element.get_text().strip():
                            source = element.get_text().strip()
                            # æ¸…ç†ä¾†æºæ–‡æœ¬
                            if "Â·" in source:
                                source = source.split("Â·")[0].strip()
                            # ç§»é™¤URLéƒ¨åˆ†
                            if "http" in source:
                                source = source.split("http")[0].strip()
                            break
                    
                    # ç¢ºä¿æ‰€æœ‰å¿…éœ€å…ƒç´ éƒ½å­˜åœ¨
                    if not title or not url:
                        continue
                    
                    # é ç¯©é¸ï¼šåªè™•ç†åŒ…å«ä¿éšªé—œéµè©çš„æ–°è
                    title_lower = title.lower()
                    insurance_terms = [
                        "ä¿éšª", "å£½éšª", "æ–°å…‰", "å°æ–°", "ç†è³ ", "ä¿å–®", 
                        "å¥åº·éšª", "æ„å¤–éšª", "é†«ç™‚éšª", "æŠ•è³‡å‹", "å¹´é‡‘"
                    ]
                    
                    if not any(term in title_lower for term in insurance_terms):
                        continue
                    
                    # è¨­ç½®é»˜èªä¾†æº
                    if not source:
                        source = "Googleæ–°è"
                    
                    # è§£æç™¼å¸ƒæ™‚é–“
                    pub_time = datetime.now()
                    
                    # æª¢æŸ¥æ™‚é–“é™åˆ¶
                    hours_diff = (datetime.now() - pub_time).total_seconds() / 3600
                    if hours_diff > self.hours_limit:
                        continue
                    
                    # æ¸…ç†æ¨™é¡Œ
                    if title:
                        title = title.replace('\n', ' ').replace('\r', ' ').strip()
                        title = ''.join(char for char in title if ord(char) < 65536)
                    
                    # ç²å–è©³ç´°å…§å®¹
                    content = self._get_article_content(url)
                    
                    # å‰µå»ºæ–°èé …ç›®
                    news_item = NewItem(
                        title=title,
                        content=content,
                        url=url,
                        published_time=pub_time,
                        source=source,
                        keyword=term
                    )
                    
                    news_items.append(news_item)
                    count += 1
                    logger.debug(f"âœ… æˆåŠŸè§£ææ–°è: {title[:30]}...")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ è§£ææ–°èæ™‚å‡ºéŒ¯: {str(e)}")
            
        except requests.RequestException as e:
            logger.error(f"âŒ è«‹æ±‚Googleæ–°èæ™‚å‡ºéŒ¯: {str(e)}")
        
        return news_items
    
    def _remove_duplicates(self, news_list: List[NewItem]) -> List[NewItem]:
        """ç§»é™¤é‡è¤‡çš„æ–°è"""
        seen_titles = set()
        seen_urls = set()
        unique_news = []
        
        for news in news_list:
            # æ¨™æº–åŒ–æ¨™é¡Œç”¨æ–¼æ¯”è¼ƒ
            normalized_title = news.title.lower().strip()
            
            # æª¢æŸ¥æ˜¯å¦é‡è¤‡
            if (normalized_title not in seen_titles and 
                news.url not in seen_urls):
                seen_titles.add(normalized_title)
                seen_urls.add(news.url)
                unique_news.append(news)
            else:
                logger.debug(f"ğŸ”„ ç§»é™¤é‡è¤‡æ–°è: {news.title[:30]}...")
        
        return unique_news
    
    def _get_article_content(self, url: str) -> str:
        """ç²å–æ–‡ç« å…§å®¹"""
        try:
            # è¨­ç½®éš¨æ©Ÿç­‰å¾…ï¼Œé¿å…è¢«ç¶²ç«™å°é–
            time.sleep(random.uniform(0.5, 1.2))
            
            response = requests.get(url, headers=self.headers, timeout=15)
            response.raise_for_status()
            
            # è‡ªå‹•æª¢æ¸¬ç·¨ç¢¼
            if response.apparent_encoding:
                response.encoding = response.apparent_encoding
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç§»é™¤è…³æœ¬å’Œæ¨£å¼æ¨™ç±¤
            for script in soup(["script", "style", "iframe", "ins", ".ad", ".ads"]):
                script.extract()
            
            # å˜—è©¦å¤šç¨®å…§å®¹é¸æ“‡å™¨
            content_selectors = [
                "div.article-content", 
                "div.article-body",
                "div.story-content", 
                "div.news-content",
                "div.content",
                "article",
                "main",
                ".post-content",
                ".entry-content",
                ".news-detail"
            ]
            
            content_text = ""
            for selector in content_selectors:
                content_element = soup.select_one(selector)
                if content_element:
                    content_text = content_element.get_text(separator=" ").strip()
                    if len(content_text) > 100:  # ç¢ºä¿å…§å®¹è¶³å¤ é•·
                        break
            
            # å¦‚æœæ‰¾ä¸åˆ°å…§å®¹ï¼Œä½¿ç”¨æ›´é€šç”¨çš„æ–¹æ³•
            if not content_text or len(content_text) < 100:
                # ç§»é™¤é ­éƒ¨ã€åº•éƒ¨ç­‰
                for element in soup.select("header, footer, nav, aside, .sidebar"):
                    element.extract()
                
                content_text = soup.get_text(separator=" ").strip()
            
            # æ¸…ç†æ–‡æœ¬
            if content_text:
                lines = [line.strip() for line in content_text.splitlines() if line.strip()]
                content_text = " ".join(lines)
                # ç§»é™¤äº‚ç¢¼å­—ç¬¦
                content_text = ''.join(char for char in content_text if ord(char) < 65536)
            
            return content_text
        
        except Exception as e:
            logger.warning(f"âš ï¸ ç²å–æ–‡ç« å…§å®¹æ™‚å‡ºéŒ¯: {str(e)}")
            return "ç„¡æ³•ç²å–æ–‡ç« å…§å®¹"
